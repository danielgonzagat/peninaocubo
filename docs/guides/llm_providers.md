# Guia de Configura√ß√£o dos Provedores de LLM

## Vis√£o Geral

O **Multi-Provider Router** do PENIN-Œ© √© uma funcionalidade poderosa que permite usar m√∫ltiplos provedores de Large Language Models (LLMs) de forma inteligente e econ√¥mica. O sistema:

- üîÄ **Roteamento Inteligente**: Seleciona automaticamente o melhor provedor baseado em custo, lat√™ncia e qualidade
- üí∞ **Gest√£o de Or√ßamento**: Controla gastos di√°rios com limites soft (95%) e hard (100%)
- üîÑ **Circuit Breaker**: Protege contra provedores inst√°veis com falhas consecutivas
- üìä **Analytics Detalhado**: Rastreia custo, lat√™ncia e taxa de sucesso por provedor
- üóÇÔ∏è **Cache L1/L2**: Cache com verifica√ß√£o de integridade HMAC-SHA256
- ‚ö° **Paraleliza√ß√£o**: Invoca m√∫ltiplos provedores em paralelo e seleciona a melhor resposta

## Provedores Suportados

O PENIN-Œ© suporta os seguintes provedores de LLM:

| Provedor | Modelos Principais | Vari√°vel de Ambiente |
|----------|-------------------|---------------------|
| **OpenAI** | GPT-4o, GPT-4 Turbo, GPT-3.5 | `OPENAI_API_KEY` |
| **Anthropic** | Claude 3.5 Sonnet, Claude 3 Opus | `ANTHROPIC_API_KEY` |
| **DeepSeek** | DeepSeek Chat, DeepSeek Coder | `DEEPSEEK_API_KEY` |
| **Mistral AI** | Mistral Large, Mistral Medium | `MISTRAL_API_KEY` |
| **Google Gemini** | Gemini 1.5 Pro, Gemini 1.5 Flash | `GEMINI_API_KEY` |
| **xAI (Grok)** | Grok Beta | `XAI_API_KEY` |

## Configura√ß√£o de Vari√°veis de Ambiente

### Passo 1: Criar arquivo `.env`

Crie um arquivo `.env` na raiz do projeto (ou copie o `.env.example`):

```bash
cp .env.example .env
```

### Passo 2: Configurar as Chaves de API

Edite o arquivo `.env` e adicione as chaves dos provedores que deseja usar:

```bash
# Providers
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxx
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxx
MISTRAL_API_KEY=xxxxxxxxxxxxxxxxxxxxx
GEMINI_API_KEY=xxxxxxxxxxxxxxxxxxxxx
XAI_API_KEY=xai-xxxxxxxxxxxxxxxxxxxxx

# Configura√ß√µes do PENIN-Œ©
PENIN_BUDGET_DAILY_USD=5.0
PENIN_MAX_PARALLEL_PROVIDERS=3
PENIN_METRICS_TOKEN=change-me
PENIN_CACHE_HMAC_KEY=change-me
```

### Passo 3: (Opcional) Configurar Modelos Espec√≠ficos

Por padr√£o, o PENIN-Œ© usa modelos otimizados para cada provedor. Voc√™ pode personalizar:

```bash
# Modelos padr√£o (opcional - j√° configurados)
OPENAI_MODEL=gpt-4o
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
DEEPSEEK_MODEL=deepseek-chat
MISTRAL_MODEL=mistral-large-latest
GEMINI_MODEL=gemini-1.5-pro
GROK_MODEL=grok-beta
```

## Obtendo as Chaves de API

### OpenAI

1. Acesse: https://platform.openai.com/api-keys
2. Fa√ßa login ou crie uma conta
3. Clique em "Create new secret key"
4. Copie a chave (come√ßa com `sk-proj-` ou `sk-`)
5. Configure billing em: https://platform.openai.com/account/billing

### Anthropic (Claude)

1. Acesse: https://console.anthropic.com/
2. Fa√ßa login ou crie uma conta
3. V√° em "API Keys" no menu lateral
4. Clique em "Create Key"
5. Copie a chave (come√ßa com `sk-ant-`)

### DeepSeek

1. Acesse: https://platform.deepseek.com/
2. Fa√ßa login ou crie uma conta
3. V√° em "API Keys"
4. Clique em "Create API Key"
5. Copie a chave (come√ßa com `sk-`)

### Mistral AI

1. Acesse: https://console.mistral.ai/
2. Fa√ßa login ou crie uma conta
3. V√° em "API Keys"
4. Clique em "Create new key"
5. Copie a chave

### Google Gemini

1. Acesse: https://makersuite.google.com/app/apikey
2. Fa√ßa login com sua conta Google
3. Clique em "Create API Key"
4. Selecione um projeto do Google Cloud (ou crie um novo)
5. Copie a chave

### xAI (Grok)

1. Acesse: https://x.ai/api
2. Fa√ßa login ou crie uma conta
3. Solicite acesso √† API (pode haver waitlist)
4. Uma vez aprovado, obtenha sua chave na console
5. Copie a chave (come√ßa com `xai-`)

## Uso B√°sico

### Exemplo 1: Uso Simples com Router

```python
import asyncio
from penin.providers.openai_provider import OpenAIProvider
from penin.providers.anthropic_provider import AnthropicProvider
from penin.router import MultiLLMRouterComplete as MultiLLMRouter

async def main():
    # Inicializar provedores
    providers = [
        OpenAIProvider(),
        AnthropicProvider(),
    ]
    
    # Criar router com or√ßamento di√°rio de $5
    router = MultiLLMRouter(providers, daily_budget_usd=5.0)
    
    # Fazer uma pergunta
    response = await router.ask(
        messages=[{"role": "user", "content": "O que √© IA ao cubo?"}],
        system="Responda de forma concisa e t√©cnica."
    )
    
    # Exibir resultado
    print(f"Provedor: {response.provider}")
    print(f"Modelo: {response.model}")
    print(f"Custo: ${response.cost_usd:.6f}")
    print(f"Tokens: {response.tokens_in + response.tokens_out}")
    print(f"Lat√™ncia: {response.latency_s:.2f}s")
    print(f"\nResposta:\n{response.content}")

if __name__ == "__main__":
    asyncio.run(main())
```

### Exemplo 2: Verificar Status do Or√ßamento

```python
# Obter status do or√ßamento
budget_status = router.get_budget_status()
print(f"Or√ßamento di√°rio: ${budget_status['daily_budget_usd']}")
print(f"Gasto hoje: ${budget_status['daily_spend_usd']:.6f}")
print(f"Restante: ${budget_status['budget_remaining_usd']:.6f}")
print(f"Uso: {budget_status['budget_used_pct']:.1f}%")
print(f"Requisi√ß√µes: {budget_status['request_count']}")
```

### Exemplo 3: Analytics por Provedor

```python
# Obter estat√≠sticas detalhadas
stats = router.get_usage_stats()

for provider_id, provider_stats in stats['providers'].items():
    print(f"\nüìä {provider_id.upper()}")
    print(f"  Requisi√ß√µes: {provider_stats['total_requests']}")
    print(f"  Taxa de sucesso: {provider_stats['success_rate']*100:.1f}%")
    print(f"  Custo m√©dio: ${provider_stats['avg_cost_per_request']:.6f}")
    print(f"  Lat√™ncia m√©dia: {provider_stats['avg_latency_s']:.3f}s")
    print(f"  Health: {provider_stats['health']}")
```

### Exemplo 4: Demo Completo

Execute o exemplo de demonstra√ß√£o inclu√≠do:

```bash
python examples/demo_router.py
```

## Gest√£o de Custos e Or√ßamento

### Configura√ß√£o de Limites

O router implementa dois n√≠veis de controle:

1. **Soft Cutoff (95%)**: Emite alerta mas continua operando
2. **Hard Cutoff (100%)**: Bloqueia novas requisi√ß√µes at√© o reset di√°rio

```python
# Configurar or√ßamento personalizado
router = MultiLLMRouter(
    providers=providers,
    daily_budget_usd=10.0,  # $10 por dia
)

# Resetar or√ßamento manualmente
router.reset_daily_budget(new_budget=15.0)
```

### Pesos de Sele√ß√£o

O router pondera tr√™s fatores ao selecionar o provedor:

```python
router = MultiLLMRouter(
    providers=providers,
    daily_budget_usd=5.0,
    cost_weight=0.4,      # 40% peso no custo (menor √© melhor)
    latency_weight=0.3,   # 30% peso na lat√™ncia (menor √© melhor)
    quality_weight=0.3,   # 30% peso na qualidade (taxa de sucesso)
)
```

### Monitoramento de Gastos

```python
# Hist√≥rico de gastos (√∫ltimos 30 dias)
budget_status = router.get_budget_status()
for day in budget_status['history']:
    print(f"{day['date']}: ${day['spend_usd']:.4f} ({day['requests']} req)")
```

## Circuit Breaker

O sistema protege contra provedores inst√°veis:

- **Threshold**: 3 falhas consecutivas ‚Üí circuit OPEN
- **Recovery**: 60 segundos de timeout
- **Half-Open**: Permite 1 tentativa para verificar recupera√ß√£o

```python
# Verificar estado dos circuit breakers
stats = router.get_usage_stats()
for provider_id, state in stats['circuit_breakers'].items():
    print(f"{provider_id}: {state}")  # healthy, degraded, unhealthy, circuit_open
```

## Cache L1/L2

O router implementa cache em dois n√≠veis com verifica√ß√£o de integridade:

- **L1 Cache**: R√°pido, m√°ximo 1.000 entradas
- **L2 Cache**: Mais lento, m√°ximo 10.000 entradas
- **TTL**: 3.600 segundos (1 hora)
- **Integridade**: HMAC-SHA256

```python
# Habilitar/desabilitar cache
router = MultiLLMRouter(
    providers=providers,
    enable_cache=True,  # Padr√£o: True
)

# Limpar cache manualmente
router.clear_cache()

# Estat√≠sticas de cache
stats = router.get_usage_stats()
cache_stats = stats['cache']
print(f"Hit rate: {cache_stats['hit_rate']*100:.1f}%")
print(f"L1 size: {cache_stats['l1_size']}")
print(f"L2 size: {cache_stats['l2_size']}")
```

## Modos de Opera√ß√£o

### Production (Padr√£o)

Modo normal de produ√ß√£o com todas as features ativas:

```python
from penin.router import RouterMode

router = MultiLLMRouter(
    providers=providers,
    mode=RouterMode.PRODUCTION,
)
```

### Dry Run

Testa a l√≥gica do router sem fazer chamadas reais:

```python
router = MultiLLMRouter(
    providers=providers,
    mode=RouterMode.DRY_RUN,
)
```

### Shadow Mode

Executa normalmente mas registra m√©tricas adicionais para an√°lise:

```python
router = MultiLLMRouter(
    providers=providers,
    mode=RouterMode.SHADOW,
)
```

## Troubleshooting

### Erro: "API Key n√£o configurada"

**Sintoma**: `ValueError: OPENAI_API_KEY is not configured`

**Solu√ß√£o**:
1. Verifique se o arquivo `.env` existe na raiz do projeto
2. Confirme que a vari√°vel est√° definida: `OPENAI_API_KEY=sk-...`
3. Reinicie o processo Python ap√≥s modificar o `.env`

### Erro: "Circuit breaker open"

**Sintoma**: `RuntimeError: Circuit breaker open for provider 'openai'`

**Solu√ß√£o**:
1. O provedor teve 3+ falhas consecutivas
2. Aguarde 60 segundos para recovery autom√°tico
3. Verifique sua API key e billing status
4. Verifique status da API do provedor: https://status.openai.com/

### Erro: "Daily budget exceeded"

**Sintoma**: `RuntimeError: Daily budget exceeded (hard cutoff)`

**Solu√ß√£o**:
1. Aumente o or√ßamento di√°rio: `PENIN_BUDGET_DAILY_USD=10.0`
2. Ou aguarde at√© meia-noite UTC para reset autom√°tico
3. Ou reset manual: `router.reset_daily_budget(new_budget=10.0)`

### Erro de autentica√ß√£o do provedor

**Sintomas**:
- OpenAI: `401 Unauthorized` ou `Incorrect API key`
- Anthropic: `authentication_error`
- Outros: Similar `401` ou `403`

**Solu√ß√£o**:
1. Verifique se a chave est√° correta (sem espa√ßos extras)
2. Confirme que a chave n√£o expirou
3. Verifique se tem cr√©ditos/billing configurado
4. Teste a chave diretamente no playground do provedor

### Provider n√£o encontrado

**Sintoma**: `ImportError: No module named 'openai'`

**Solu√ß√£o**:
```bash
# Instalar depend√™ncias dos provedores
pip install openai anthropic google-generativeai mistralai
```

### Performance lenta

**Sintomas**: Alta lat√™ncia nas respostas

**Solu√ß√£o**:
1. Reduza n√∫mero de provedores paralelos:
   ```bash
   PENIN_MAX_PARALLEL_PROVIDERS=2
   ```
2. Habilite cache se desabilitado
3. Considere usar modelos mais r√°pidos (ex: gpt-4o-mini)

## Configura√ß√µes Avan√ßadas

### Paraleliza√ß√£o Limitada

```bash
# Limitar n√∫mero de provedores consultados em paralelo
PENIN_MAX_PARALLEL_PROVIDERS=2
```

### Persist√™ncia de Estado

O router salva estado automaticamente em:
- `~/.penin_router_complete_state.json`

Inclui:
- Or√ßamento e hist√≥rico
- Estat√≠sticas por provedor
- Estat√≠sticas de cache

### HMAC Cache Secret

Para ambientes de produ√ß√£o, configure uma chave HMAC personalizada:

```bash
PENIN_CACHE_HMAC_KEY=seu-segredo-super-secreto-aqui
```

### Integra√ß√£o com M√©tricas

O router est√° pronto para integra√ß√£o com Prometheus:

```python
# Obter m√©tricas completas
analytics = router.get_analytics()

# Exportar para Prometheus (implementar exportador)
# prometheus_metrics.gauge('penin_budget_used_pct', analytics['budget_used_pct'])
# prometheus_metrics.gauge('penin_provider_health', analytics['providers'][pid]['success_rate'])
```

## Boas Pr√°ticas

### 1. Sempre Configure M√∫ltiplos Provedores

Use pelo menos 2-3 provedores para garantir alta disponibilidade:

```python
providers = [
    OpenAIProvider(),      # Prim√°rio
    AnthropicProvider(),   # Fallback 1
    DeepSeekProvider(),    # Fallback 2 (custo-benef√≠cio)
]
```

### 2. Monitore o Or√ßamento

Configure alertas quando atingir o soft cutoff (95%):

```python
budget = router.get_budget_status()
if budget['soft_cutoff_reached']:
    print("‚ö†Ô∏è  ALERTA: Or√ßamento em 95%!")
```

### 3. Use Cache para Queries Repetidas

Perfeito para cen√°rios com perguntas similares:

```python
# Cache ativo por padr√£o
response = await router.ask(messages, use_cache=True)
```

### 4. Ajuste Pesos Conforme Necessidade

Para priorizar custo:
```python
router = MultiLLMRouter(
    providers=providers,
    cost_weight=0.6,      # 60% peso no custo
    latency_weight=0.2,   # 20% peso na lat√™ncia
    quality_weight=0.2,   # 20% peso na qualidade
)
```

Para priorizar velocidade:
```python
router = MultiLLMRouter(
    providers=providers,
    cost_weight=0.1,      # 10% peso no custo
    latency_weight=0.6,   # 60% peso na lat√™ncia
    quality_weight=0.3,   # 30% peso na qualidade
)
```

### 5. Teste em Dry Run Primeiro

Antes de deployar mudan√ßas:

```python
router = MultiLLMRouter(
    providers=providers,
    mode=RouterMode.DRY_RUN,  # Sem chamadas reais
)
```

## Recursos Adicionais

- **C√≥digo do Router**: `penin/router.py` e `penin/router_complete.py`
- **Provedores**: `penin/providers/`
- **Exemplo Completo**: `examples/demo_router.py`
- **Documenta√ß√£o da Arquitetura**: `docs/architecture.md`
- **README Principal**: `README.md`

## Suporte e Comunidade

- **Issues**: https://github.com/danielgonzagat/peninaocubo/issues
- **Discuss√µes**: https://github.com/danielgonzagat/peninaocubo/discussions
- **Contributing**: Veja `CONTRIBUTING.md`

---

**üöÄ Pronto!** Agora voc√™ est√° preparado para usar o Multi-Provider Router do PENIN-Œ© com total controle sobre custos, performance e confiabilidade.
