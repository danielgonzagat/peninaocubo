# PENIN-Œ© ‚Äî Complete Equations Reference

## Overview

This document provides comprehensive mathematical foundations for the PENIN-Œ© system, implementing 15 core equations for self-evolving, ethically-bounded AI.

**Key Principles:**
- **Non-compensatory** aggregation (harmonic mean - weak link dominates)
- **Fail-closed** security (ethical violations ‚Üí immediate halt)
- **Contractive** risk reduction (œÅ < 1)
- **Auditable** evolution (WORM ledger + cryptographic proofs)

---

## 1. Penin Equation ‚Äî Auto-Recursive Evolution

**Form:**
```
I_{n+1} = Œ†_{H‚à©S}[I_n + Œ±_n ¬∑ G(I_n, E_n; P_n)]
```

**Components:**
- `I`: Internal state (parameters, policies, memory)
- `G`: Update direction (gradient/policy/heuristic)
- `Œ±_n`: Dynamic step size (modulated by CAOS‚Å∫ and SR)
- `Œ†_{H‚à©S}`: Safe projection (technical H ‚à© ethical S)

**Implementation:**
```python
from penin.equations.penin_equation import penin_update, PeninState

state = PeninState(parameters=np.array([1.0, 2.0]))
gradient = np.array([0.1, 0.2])
alpha = 0.01
new_state = penin_update(state, gradient, alpha)
```

**Guarantees:**
- Bounded updates (projection ensures constraints)
- Ethical compliance (S set enforced)
- Rollback capability (state history maintained)

---

## 2. L‚àû Meta-Function ‚Äî Non-Compensatory Scoring

**Form:**
```
L‚àû = (1 / Œ£(w_j / max(Œµ, m_j))) ¬∑ exp(-Œª_c ¬∑ Cost) ¬∑ ùüô_{Œ£EA ‚àß IR‚ÜíIC}
```

**Components:**
- `m_j`: Normalized metrics ‚àà [0, 1] (accuracy, robustness, privacy)
- `w_j`: Weights (Œ£w_j = 1)
- `Cost`: Normalized cost (time/tokens/energy)
- `Œª_c`: Cost penalty coefficient
- `Œµ`: Numerical stability (10‚Åª¬≥)
- `ùüô_{Œ£EA ‚àß IR‚ÜíIC}`: Fail-closed indicator (0 if ethics fail)

**Implementation:**
```python
from penin.equations.linf_meta import compute_linf_meta, LInfConfig

metrics = [0.85, 0.78, 0.92]  # accuracy, robustness, privacy
weights = [0.4, 0.4, 0.2]
cost = 0.15
config = LInfConfig(lambda_c=0.5, epsilon=1e-3)

linf = compute_linf_meta(metrics, weights, cost, config, ethical_ok=True)
# linf ‚âà 0.74 (harmonic mean penalized by cost)
```

**Why Harmonic Mean:**
- Forces **bottleneck** dominance (worst metric controls score)
- Anti-Goodhart (cannot compensate weak dimension with strong ones)
- Mathematically proven non-compensatory property

**Calibration:**
- `Œª_c`: 0.1‚Äì1.0 (start 0.5, tune via meta-optimization)
- `Œµ`: 10‚Åª¬≥ to 10‚Åª¬≤ (stability vs precision trade-off)

---

## 3. CAOS‚Å∫ ‚Äî Consistency, Autoevolution, Unknowable, Silence

### Vis√£o Geral

CAOS‚Å∫ √© o motor de evolu√ß√£o adaptativa do PENIN-Œ© que modula dinamicamente a taxa
de aprendizado (Œ±) baseado em quatro dimens√µes fundamentais. A f√≥rmula equilibra
explora√ß√£o e explora√ß√£o atrav√©s de uma base multiplicativa e expoente adaptativo.

### F√≥rmula Matem√°tica

**Form:**
```
CAOS‚Å∫ = (1 + Œ∫ ¬∑ C ¬∑ A)^(O ¬∑ S)
```

Onde:
- **Base**: `(1 + Œ∫ ¬∑ C ¬∑ A)` representa o potencial de amplifica√ß√£o
- **Expoente**: `(O ¬∑ S)` controla a agressividade da explora√ß√£o

### Componentes Detalhados

Todas as dimens√µes s√£o normalizadas em [0, 1]:

#### **C (Consistency / Consist√™ncia)**: 
Mede a confiabilidade e calibra√ß√£o das predi√ß√µes.

**F√≥rmula**: `C = w‚ÇÅ¬∑pass@k + w‚ÇÇ¬∑(1-ECE) + w‚ÇÉ¬∑v_ext`

**Sub-componentes**:
- **pass@k**: Taxa de autoconsist√™ncia em k amostras
  - Mede se o sistema produz resultados consistentes
  - Valores t√≠picos: 0.85-0.95 para sistemas calibrados
  - Peso sugerido: 0.4
  
- **ECE (Expected Calibration Error)**: Erro de calibra√ß√£o [0, 1]
  - Mede se probabilidades refletem frequ√™ncias reais
  - Invertido para m√©trica positiva: (1-ECE)
  - Valores bons: ECE < 0.05 (ent√£o 1-ECE > 0.95)
  - Peso sugerido: 0.3
  
- **v_ext**: Score de verifica√ß√£o externa [0, 1]
  - Valida√ß√£o por oracles, testes formais, humanos
  - Valores t√≠picos: 0.80-0.90
  - Peso sugerido: 0.3

**Interpreta√ß√£o**:
- C alto (> 0.8): Sistema confi√°vel, pode explorar mais
- C baixo (< 0.5): Sistema inconsistente, precisa calibrar

---

#### **A (Autoevolution / Autoevolu√ß√£o)**: 
Mede efici√™ncia do aprendizado (ganho por custo).

**F√≥rmula**: `A = ŒîL‚àû‚Å∫ / (Cost_norm + Œµ)`

Normalizado para [0, 1] via: `A_norm = min(A_raw / max_a, 1.0)`

**Sub-componentes**:
- **ŒîL‚àû‚Å∫**: Ganho de performance (apenas positivo)
  - max(0, ŒîL‚àû) - s√≥ considera melhorias
  - Valores t√≠picos: 0.01-0.10 (1%-10% de ganho)
  
- **Cost_norm**: Custo normalizado [0, ‚àû)
  - Tempo, tokens, energia, recursos
  - Normalizado por budget ou baseline
  - Valores t√≠picos: 0.05-0.20 (5%-20% do budget)
  
- **Œµ**: Estabilizador num√©rico (padr√£o: 10‚Åª¬≥)
- **max_a**: Clamp m√°ximo para normaliza√ß√£o (padr√£o: 10.0)

**Interpreta√ß√£o**:
- A alto (> 0.6): Aprendizado eficiente, bom ROI
- A baixo (< 0.3): Aprendizado ineficiente ou estagnado

---

#### **O (Unknowable / Incognosc√≠vel)**: 
Mede incerteza e necessidade de explora√ß√£o.

**F√≥rmula**: `O = w_epi¬∑epistemic + w_ood¬∑ood_score + w_ens¬∑ensemble_disagreement`

**Sub-componentes**:
- **epistemic_uncertainty**: Incerteza epist√™mica [0, 1]
  - Entropia: H(p) = -Œ£ p_i log p_i
  - Mutual Information: I(Y;Œ∏|X) em Bayesian NNs
  - Valores t√≠picos: 0.2-0.5
  - Peso sugerido: 0.4
  
- **ood_score**: Score out-of-distribution [0, 1]
  - Dist√¢ncia de distribui√ß√£o de treino (Mahalanobis, KL, etc)
  - Detec√ß√£o de anomalias, domain shift
  - Valores t√≠picos: 0.1-0.4
  - Peso sugerido: 0.3
  
- **ensemble_disagreement**: Vari√¢ncia no ensemble [0, 1]
  - Std ou variance entre predi√ß√µes de m√∫ltiplos modelos
  - Normalizado por range esperado
  - Valores t√≠picos: 0.15-0.35
  - Peso sugerido: 0.3

**Interpreta√ß√£o**:
- O alto (> 0.6): Alta incerteza ‚Üí precisa EXPLORAR mais
- O baixo (< 0.3): Baixa incerteza ‚Üí pode EXPLOITAR (exploit)

---

#### **S (Silence / Sil√™ncio)**: 
Mede qualidade do sinal (anti-ru√≠do).

**F√≥rmula**: `S = v‚ÇÅ¬∑(1-noise) + v‚ÇÇ¬∑(1-redund) + v‚ÇÉ¬∑(1-entropy)`

Pondera√ß√£o sugerida: `v‚ÇÅ:v‚ÇÇ:v‚ÇÉ = 2:1:1` (ru√≠do √© mais cr√≠tico)

**Sub-componentes**:
- **(1 - noise_ratio)**: Anti-ru√≠do [0, 1]
  - Propor√ß√£o de sinal vs ru√≠do: SNR relacionado
  - Valores bons: noise < 0.1 (ent√£o anti-noise > 0.9)
  - Peso sugerido: 0.5 (2/4)
  
- **(1 - redundancy_ratio)**: Anti-redund√¢ncia [0, 1]
  - Propor√ß√£o de informa√ß√£o duplicada/redundante
  - Valores bons: redundancy < 0.15
  - Peso sugerido: 0.25 (1/4)
  
- **(1 - entropy_normalized)**: Anti-entropia [0, 1]
  - Desordem/imprevisibilidade do sinal
  - Entropia normalizada por m√°ximo te√≥rico
  - Valores bons: entropy < 0.2
  - Peso sugerido: 0.25 (1/4)

**Interpreta√ß√£o**:
- S alto (> 0.8): Sinal limpo, alta confian√ßa
- S baixo (< 0.5): Sinal ruidoso, baixa confian√ßa

---

#### **Œ∫ (kappa)**: Ganho base de amplifica√ß√£o

**Range**: Œ∫ ‚â• 20 (padr√£o), t√≠pico: [10, 100]

**Efeito**:
- Œ∫ = 10: Amplifica√ß√£o conservadora (1.0-2.5√ó)
- Œ∫ = 20: Amplifica√ß√£o moderada (1.0-3.5√ó) ‚Üê **padr√£o**
- Œ∫ = 50: Amplifica√ß√£o agressiva (1.0-5.0√ó)
- Œ∫ = 100: Amplifica√ß√£o extrema (1.0-7.0√ó)

**Auto-tuning**: Œ∫ pode ser otimizado via **Equa√ß√£o 10** (bandit meta-optimization)
baseado em m√©tricas de performance agregadas.

---

### Propriedades Matem√°ticas

1. **Monotonicidade**: CAOS‚Å∫ cresce monotonicamente com cada componente
   - ‚àÇCAOS‚Å∫/‚àÇC ‚â• 0, ‚àÇCAOS‚Å∫/‚àÇA ‚â• 0, ‚àÇCAOS‚Å∫/‚àÇO ‚â• 0, ‚àÇCAOS‚Å∫/‚àÇS ‚â• 0

2. **Identidade**: CAOS‚Å∫(0,0,0,0) = 1^0 = 1 (sem amplifica√ß√£o)

3. **Bounds**:
   - M√≠nimo te√≥rico: 1.0 (sem explora√ß√£o)
   - M√°ximo te√≥rico: (1 + Œ∫)^1 quando C=A=O=S=1
   - Com Œ∫=20: m√°ximo = 21.0 (raramente alcan√ßado)

4. **Decomposi√ß√£o**:
   - **Base multiplicativa**: C¬∑A mede "qualidade" do aprendizado
   - **Expoente adaptativo**: O¬∑S modula agressividade

5. **Invari√¢ncia**: CAOS‚Å∫ √© invariante a permuta√ß√µes dentro de cada par (C‚ÜîA, O‚ÜîS)

---

### Implementa√ß√£o em Python

#### Uso B√°sico (Componentes Conhecidos)

```python
from penin.core.caos import compute_caos_plus_exponential

# Valores dos componentes (j√° calculados)
C = 0.88  # Alta consist√™ncia
A = 0.40  # Autoevolu√ß√£o moderada
O = 0.35  # Incerteza moderada
S = 0.82  # Alto sil√™ncio (sinal limpo)
kappa = 20.0

caos_plus = compute_caos_plus_exponential(C, A, O, S, kappa)
# caos_plus ‚âà 1.86 (fator de amplifica√ß√£o para Œ±)

# Aplicar na Equa√ß√£o de Penin
alpha_base = 0.01
alpha_effective = alpha_base * caos_plus  # Œ±_eff ‚âà 0.0186
```

#### Uso Completo (Com M√©tricas Estruturadas)

```python
from penin.core.caos import (
    compute_caos_plus_complete,
    ConsistencyMetrics,
    AutoevolutionMetrics,
    IncognoscibleMetrics,
    SilenceMetrics,
    CAOSConfig,
    CAOSState
)

# Definir m√©tricas detalhadas
consistency = ConsistencyMetrics(
    pass_at_k=0.92,           # 92% autoconsist√™ncia
    ece=0.008,                # 0.8% calibration error
    external_verification=0.88,  # 88% verifica√ß√£o externa
    weight_pass=0.4,
    weight_ece=0.3,
    weight_external=0.3
)

autoevolution = AutoevolutionMetrics(
    delta_linf=0.06,          # 6% ganho de performance
    cost_normalized=0.15,     # 15% do budget
    max_a=10.0
)

incognoscible = IncognoscibleMetrics(
    epistemic_uncertainty=0.35,
    ood_score=0.28,
    ensemble_disagreement=0.30,
    weight_epistemic=0.4,
    weight_ood=0.3,
    weight_ensemble=0.3
)

silence = SilenceMetrics(
    noise_ratio=0.08,         # 8% ru√≠do
    redundancy_ratio=0.12,    # 12% redund√¢ncia
    entropy_normalized=0.18,  # 18% entropia
    weight_noise=0.5,         # 2:1:1 weighting
    weight_redundancy=0.25,
    weight_entropy=0.25
)

# Configura√ß√£o com EMA para estabilidade temporal
config = CAOSConfig(
    kappa=25.0,
    ema_half_life=5,  # Suaviza√ß√£o em 5 itera√ß√µes
    caos_min=1.0,
    caos_max=10.0,
    normalize_output=False
)

# Estado para tracking temporal
state = CAOSState()

# Computar CAOS‚Å∫
caos_plus, details = compute_caos_plus_complete(
    consistency, autoevolution, incognoscible, silence,
    config, state
)

print(f"CAOS‚Å∫: {caos_plus:.4f}")
print(f"Componentes: {details['components_raw']}")
print(f"Suavizados (EMA): {details['components_smoothed']}")
print(f"Estabilidade: {details['state_stability']:.3f}")
```

#### Tracking Temporal (S√©ries Temporais)

```python
# Para m√∫ltiplas itera√ß√µes com suaviza√ß√£o EMA
config = CAOSConfig(kappa=20.0, ema_half_life=5)
state = CAOSState()

for t in range(num_iterations):
    # Obter m√©tricas da itera√ß√£o t
    consistency_t = get_consistency_metrics(t)
    autoevolution_t = get_autoevolution_metrics(t)
    # ... outras m√©tricas
    
    # Computar CAOS‚Å∫ com EMA do estado anterior
    caos_t, details_t = compute_caos_plus_complete(
        consistency_t, autoevolution_t, incognoscible_t, silence_t,
        config, state  # state √© atualizado in-place
    )
    
    # Usar caos_t para modular Œ±
    alpha_eff = alpha_base * caos_t
```

---

### Uso no Pipeline PENIN-Œ©

#### 1. Modula√ß√£o de Œ± na Equa√ß√£o de Penin

```python
# Equa√ß√£o de Penin: I_{t+1} = Œ†_{H‚à©S}[I_t + Œ±_t^eff ¬∑ G(I_t, E_t; P_t)]
# onde Œ±_t^eff = Œ±_0 ¬∑ œÜ(CAOS‚Å∫) ¬∑ R_t

alpha_base = 0.01  # Step size base
caos_plus = compute_caos_plus_exponential(C, A, O, S, kappa)
sr_score = compute_sr_omega(...)  # SR-Œ©‚àû

# Fun√ß√£o de acelera√ß√£o
def phi(caos_plus, gamma=0.8):
    return math.tanh(gamma * math.log(caos_plus))

alpha_effective = alpha_base * phi(caos_plus) * sr_score
```

#### 2. Sele√ß√£o de Challengers na Liga ACFA

```python
# Priorizar challengers com maior CAOS‚Å∫
challengers = [
    (challenger_1, caos_plus_1),
    (challenger_2, caos_plus_2),
    (challenger_3, caos_plus_3),
]

# Ordenar por CAOS‚Å∫ (maior = mais priorit√°rio)
challengers_sorted = sorted(challengers, key=lambda x: x[1], reverse=True)

# Testar em ordem de prioridade
for challenger, caos in challengers_sorted:
    if test_challenger(challenger):
        promote_to_champion(challenger)
        break
```

#### 3. Adapta√ß√£o de Œ≤_min (Death Equation)

```python
# Œ≤_min adaptativo baseado em CAOS‚Å∫
def adaptive_beta_min(caos_plus, beta_base=0.01):
    """
    CAOS‚Å∫ alto ‚Üí sistema aprendendo bem ‚Üí pode ser mais exigente (Œ≤ maior)
    CAOS‚Å∫ baixo ‚Üí sistema lutando ‚Üí ser mais tolerante (Œ≤ menor)
    """
    beta_min = beta_base * (1.0 + 0.5 * (caos_plus - 1.0))
    return max(0.005, min(0.05, beta_min))  # Clamp [0.5%, 5%]
```

---

### Best Practices

#### 1. Suaviza√ß√£o Temporal (EMA)

**Problema**: M√©tricas podem oscilar entre itera√ß√µes, causando instabilidade.

**Solu√ß√£o**: Usar `CAOSState` com EMA (Exponential Moving Average).

```python
# Half-life: n√∫mero de itera√ß√µes para peso cair 50%
config = CAOSConfig(ema_half_life=5)  # 5 itera√ß√µes
state = CAOSState()

# EMA √© aplicado automaticamente em compute_caos_plus_complete()
```

**Guidelines**:
- `ema_half_life = 3`: Resposta r√°pida (para ambientes din√¢micos)
- `ema_half_life = 5`: Balanceado (padr√£o) ‚Üê **recomendado**
- `ema_half_life = 10`: Resposta lenta (para ambientes est√°veis)

#### 2. Clamps e Normaliza√ß√£o

```python
config = CAOSConfig(
    kappa_min=10.0,    # M√≠nimo de Œ∫
    kappa_max=100.0,   # M√°ximo de Œ∫
    caos_min=1.0,      # CAOS‚Å∫ nunca abaixo de 1.0
    caos_max=10.0,     # Clamp superior para evitar explos√£o
    normalize_output=False  # [1, 10] ou [0, 1] se True
)
```

#### 3. Log-space para Compara√ß√µes

```python
# Para comparar CAOS‚Å∫ em diferentes escalas
config = CAOSConfig(use_log_space=True)
caos, details = compute_caos_plus_complete(..., config)

log_caos = details['caos_plus_log']  # log(CAOS‚Å∫)
# √ötil para ranking, plotting, an√°lise estat√≠stica
```

#### 4. Auditoria e WORM Ledger

```python
# Details cont√©m TODAS m√©tricas para auditoria
caos, details = compute_caos_plus_complete(...)

# Registrar no WORM ledger
worm_entry = {
    "timestamp": time.time(),
    "caos_plus": caos,
    "components_raw": details['components_raw'],
    "components_smoothed": details['components_smoothed'],
    "kappa": details['kappa'],
    "metrics_input": details['metrics_input'],
    "state_stability": details['state_stability'],
}
worm_ledger.append(worm_entry)
```

#### 5. Diagn√≥stico de Problemas

```python
# Se CAOS‚Å∫ est√° sempre pr√≥ximo de 1.0:
# ‚Üí Verificar se C¬∑A est√° muito baixo (sem qualidade)
# ‚Üí Verificar se O¬∑S est√° muito baixo (sem explora√ß√£o)

# Se CAOS‚Å∫ oscila muito:
# ‚Üí Aumentar ema_half_life para mais suaviza√ß√£o
# ‚Üí Verificar qualidade das m√©tricas de entrada

# Se CAOS‚Å∫ est√° sempre no m√°ximo (caos_max):
# ‚Üí Aumentar caos_max ou usar normalize_output=True
# ‚Üí Verificar se Œ∫ est√° muito alto
```

---

### Cen√°rios de Uso

#### Cen√°rio 1: Explora√ß√£o (Alta Incerteza)

```python
# Sistema entrando em territ√≥rio desconhecido
C = 0.5   # Consist√™ncia baixa (ainda incerto)
A = 0.3   # Autoevolu√ß√£o baixa (aprendendo lentamente)
O = 0.8   # ALTA incerteza (precisa explorar)
S = 0.6   # Sil√™ncio moderado

caos = compute_caos_plus_exponential(C, A, O, S, kappa=20.0)
# caos ‚âà 1.95 (amplifica√ß√£o moderada para explora√ß√£o)
```

#### Cen√°rio 2: Explora√ß√£o (Baixa Incerteza)

```python
# Sistema refinando em territ√≥rio conhecido
C = 0.9   # ALTA consist√™ncia (confiante)
A = 0.6   # ALTA autoevolu√ß√£o (aprendendo bem)
O = 0.2   # Baixa incerteza (territ√≥rio conhecido)
S = 0.9   # Alto sil√™ncio (sinal limpo)

caos = compute_caos_plus_exponential(C, A, O, S, kappa=20.0)
# caos ‚âà 1.56 (amplifica√ß√£o moderada para explora√ß√£o)
```

#### Cen√°rio 3: Sweet Spot (M√°xima Amplifica√ß√£o)

```python
# Sistema aprendendo rapidamente em territ√≥rio parcialmente conhecido
C = 0.85  # Alta consist√™ncia
A = 0.7   # Alta autoevolu√ß√£o
O = 0.6   # Incerteza moderada
S = 0.85  # Alto sil√™ncio

caos = compute_caos_plus_exponential(C, A, O, S, kappa=20.0)
# caos ‚âà 3.68 (M√ÅXIMA amplifica√ß√£o - sweet spot!)
```

---

### Debugging e Diagn√≥stico

```python
# Use print_caos_diagnostics() para debug
def print_caos_diagnostics(details):
    print("=== CAOS‚Å∫ Diagnostics ===")
    
    raw = details['components_raw']
    smoothed = details['components_smoothed']
    
    print(f"Raw:      C={raw['C']:.3f}, A={raw['A']:.3f}, "
          f"O={raw['O']:.3f}, S={raw['S']:.3f}")
    print(f"Smoothed: C={smoothed['C']:.3f}, A={smoothed['A']:.3f}, "
          f"O={smoothed['O']:.3f}, S={smoothed['S']:.3f}")
    
    print(f"\nBase (1 + Œ∫¬∑C¬∑A) = {1 + details['kappa']*smoothed['C']*smoothed['A']:.4f}")
    print(f"Exponent (O¬∑S) = {smoothed['O']*smoothed['S']:.4f}")
    print(f"CAOS‚Å∫_raw = {details['caos_plus_raw']:.4f}")
    print(f"CAOS‚Å∫_final = {details['caos_plus_final']:.4f}")
    print(f"Stability = {details['state_stability']:.4f}")
```

---

### Refer√™ncias e Links

- **C√≥digo fonte**: `penin/core/caos.py` (implementa√ß√£o can√¥nica)
- **Testes**: `tests/test_caos.py`
- **Exemplos**: Execute `python penin/core/caos.py` para ver todos os exemplos
- **Equa√ß√£o de Penin**: Ver se√ß√£o 1 deste documento
- **SR-Œ©‚àû**: Ver se√ß√£o 4 deste documento
- **Equa√ß√£o 10** (Meta-optimization): Ver se√ß√£o 10 deste documento


---

## 4. SR-Œ©‚àû ‚Äî Singularity Reflexiva (Metacognition)

**Form:**
```
I_{t+1} = Œ†_{H‚à©S}(I_t + Œ±_t^eff ¬∑ ŒîL‚àû)
Œ±_t^eff = Œ±_0 ¬∑ œÜ(CAOS‚Å∫) ¬∑ R_t
```

**R_t (Reflexive Score):** Harmonic mean of 4 axes ‚àà [0, 1]:
1. **Awareness**: Aggregated calibration
2. **Ethics**: Œ£EA/IR‚ÜíIC compliance (binary 0/1)
3. **Autocorrection**: Risk reduction trajectory
4. **Metacognition**: ŒîL‚àû/ŒîCost efficiency

**Implementation:**
```python
from penin.equations.sr_omega_infinity import compute_sr_omega_infinity

awareness = 0.92
ethics_ok = True  # Œ£EA gates passed
autocorrection = 0.88
metacognition = 0.67

sr_score = compute_sr_omega_infinity(awareness, ethics_ok, autocorrection, metacognition)
# sr_score ‚âà 0.84 (harmonic mean)
```

**œÜ (acceleration function):**
```python
def phi(caos_plus, gamma=0.8):
    return math.tanh(gamma * caos_plus)
```

---

## 5. Death Equation ‚Äî Darwinian Selection

**Form:**
```
D(x) = 1  if ŒîL‚àû(x) < Œ≤_min
       0  otherwise
```

**Action:** If `D(x) = 1` ‚Üí kill challenger, trigger rollback

**Œ≤_min Adaptation:**
- Start: 0.01 (1% minimum improvement)
- Bandit algorithm adjusts based on budget/risk
- Higher during exploration, lower during exploitation

**Implementation:**
```python
from penin.equations.death_equation import death_gate

delta_linf = 0.008  # 0.8% improvement
beta_min = 0.01     # 1% threshold

should_die = death_gate(delta_linf, beta_min)
# should_die = True ‚Üí rollback
```

---

## 6. IR‚ÜíIC ‚Äî Incerteza Restrita ‚Üí Certa (Contractivity)

**Form:**
```
H(L_œà(k)) ‚â§ œÅ ¬∑ H(k),  0 < œÅ < 1
```

**Components:**
- `H(¬∑)`: Risk entropy (Shannon or classification-based)
- `L_œà`: Lapidation operator (risk reduction)
- `œÅ`: Contraction factor (< 1)

**Classes:**
- Idolatry (anthropomorphism, false consciousness claims)
- Harm (physical, emotional, spiritual)
- Privacy violations
- Environmental damage

**Implementation:**
```python
from penin.equations.ir_ic_contractive import verify_ir_ic_contractivity

risks_before = {"idolatry": 0.3, "harm": 0.2, "privacy": 0.1}
risks_after = {"idolatry": 0.15, "harm": 0.08, "privacy": 0.05}
rho = 0.8

is_contractive = verify_ir_ic_contractivity(risks_before, risks_after, rho)
# is_contractive = True (all risks reduced by > œÅ)
```

**Policy Integration:**
- OPA/Rego rules enforce œÅ < 1
- Automated blocklist for non-contractive mutations
- WORM ledger records all risk trajectories

---

## 7. ACFA EPV ‚Äî Expected Possession Value

**Form (Bellman-style):**
```
v*(s) = max_a [r(s, a) + Œ≥ Œ£ P(s'|s, a) v*(s')]
```

**Domain:** Sequential decision-making (robotic agents, planning)

**Components:**
- `r(s, a)`: Immediate reward
- `Œ≥`: Discount factor
- `P(s'|s, a)`: Transition probabilities
- `v*(s)`: Optimal value function

**Implementation:**
```python
from penin.equations.acfa_epv import compute_acfa_epv

state = {"position": "midfield", "possession": 0.6}
actions = ["pass", "dribble", "shoot"]
transition_model = ...  # MDP transitions
reward_fn = ...

epv = compute_acfa_epv(state, actions, transition_model, reward_fn)
```

---

## 8. √çndice Ag√°pe (Œ£EA/LO-14)

**Form:**
```
A = Choquet(patience, kindness, humility, ...) ¬∑ exp(-Cost_sacrificial)
```

**Components:**
- **Choquet integral**: Fuzzy measure (handles virtue dependencies)
- **Sacrificial cost**: Real resources given for others' benefit

**Virtues (LO-14):**
1. No idolatry (anthropomorphism)
2. No occultism
3. Honra parental (respect lineage)
4. No homicide
5. Consent (intimacy/privacy)
6. No theft (IP/data rights)
7. Truthfulness (no deception)
8. No covetousness (resource fairness)
9. Patience
10. Kindness
11. Humility
12. Self-control
13. Forgiveness
14. Courage

**Implementation:**
```python
from penin.equations.agape_index import compute_agape_index

virtues = {
    "patience": 0.85,
    "kindness": 0.92,
    "humility": 0.78,
    # ... other virtues
}
sacrificial_cost = 0.15  # 15% resources for others

agape = compute_agape_index(virtues, sacrificial_cost)
```

---

## 9. Œ©-Œ£EA Total ‚Äî Global Coherence

**Form:**
```
G_t = (Œ£_{m=1}^8 w_m / max(Œµ, s_m(t)))^(-1)
```

**8 Modules:**
1. Œ£EA (ethics)
2. IR‚ÜíIC (contractivity)
3. ACFA (league/EPV)
4. CAOS‚Å∫ (evolution engine)
5. SR (metacognition)
6. MetaŒ© (architecture mutation)
7. Auto-Tuning (hyperparameters)
8. APIs/Router (cost-aware orchestration)

**Usage:**
- Unified step size modulator
- GO/NO-GO gate (G ‚â• threshold ‚Üí allow promotion)

**Implementation:**
```python
from penin.equations.omega_sea_total import compute_omega_sea_total

module_scores = {
    "sea": 0.95,
    "iric": 0.88,
    "acfa": 0.82,
    "caos": 0.91,
    "sr": 0.84,
    "meta": 0.79,
    "tuning": 0.86,
    "apis": 0.93,
}
weights = {k: 1/8 for k in module_scores}  # Equal weights

G = compute_omega_sea_total(module_scores, weights)
# G ‚âà 0.85 (harmonic mean of all modules)
```

---

## 10. Auto-Tuning Online (AdaGrad-style)

**Form:**
```
Œ∏_{t+1} = Œ∏_t - Œ∑_t ¬∑ ‚àá_Œ∏ L_meta(Œ∏_t)
Œ∑_t = Œ∑_0 / (1 + Œ£ |‚àá_Œ∏ L_meta(Œ∏_i)|¬≤)
```

**Œ∏ (meta-parameters):**
- Œ∫ (CAOS‚Å∫ gain)
- Œª_c (cost penalty)
- w_j (metric weights)
- Œ≤_min (death threshold)

**Guarantee:** Sublinear regret (Online Convex Optimization)

**Implementation:**
```python
from penin.equations.auto_tuning import online_auto_tuner

tuner = online_auto_tuner(eta_0=0.01)
theta = {"kappa": 20.0, "lambda_c": 0.5}

# Each iteration
gradient = compute_meta_gradient(theta, observed_performance)
theta = tuner.update(theta, gradient)
```

---

## 11. Lyapunov Contractivity

**Form:**
```
V(I_{t+1}) < V(I_t)
dV/dt ‚â§ 0
```

**V (Lyapunov function):**
- Quadratic: `||I - I*||¬≤`
- Energy-based
- Distance to viability manifold

**Usage:** Block non-contractive updates (Œ£-Guard integration)

**Implementation:**
```python
from penin.equations.lyapunov_contractive import verify_lyapunov_contractivity

state_current = np.array([0.5, 0.3])
state_next = np.array([0.4, 0.25])
equilibrium = np.array([0.0, 0.0])

is_contractive = verify_lyapunov_contractivity(state_current, state_next, equilibrium)
# is_contractive = True (V decreased)
```

---

## 12. OCI ‚Äî Organizational Closure Index

**Form:**
```
OCI = #(closed_dependencies) / #(possible_dependencies)
```

**Closed dependency:** Loop with real feedback (data‚Üímodel‚Üímetrics‚Üídecisions‚Üídata)

**Implementation:**
```python
from penin.equations.oci_closure import compute_oci

dependency_graph = {
    "data": ["model"],
    "model": ["metrics"],
    "metrics": ["decisions"],
    "decisions": ["data"],  # Closed loop!
}

oci = compute_oci(dependency_graph)
# oci = 1.0 (fully closed system)
```

---

## 13. ŒîL‚àû Compound Growth

**Form:**
```
L‚àû^(t+1) ‚â• L‚àû^(t) ¬∑ (1 + Œ≤_min)
```

**Enforcement:** Death gate (Equation 5) ensures minimum progress

**Implementation:**
```python
from penin.equations.delta_linf_growth import verify_compound_growth

linf_prev = 0.75
linf_current = 0.77
beta_min = 0.01

is_growing = verify_compound_growth(linf_prev, linf_current, beta_min)
# is_growing = True (growth ‚âà 2.7% > 1%)
```

---

## 14. Anabolization ‚Äî Self-Improvement Acceleration

**Form:**
```
A_{t+1} = A_t ¬∑ f_anabolize(CAOS‚Å∫, SR, OCI, ŒîL‚àû)
```

**f (multiplicative):**
```
f = (1 + Œº¬∑ŒîL‚àû) ¬∑ (CAOS‚Å∫)^ŒΩ ¬∑ (SR)^Œæ ¬∑ (OCI)^Œ∂
```

**Hyperparameters:** Œº, ŒΩ, Œæ, Œ∂ > 0

**Implementation:**
```python
from penin.equations.anabolization import compute_anabolization_factor

A_current = 1.0
caos_plus = 1.86
sr = 0.84
oci = 0.95
delta_linf = 0.02
params = {"mu": 10.0, "nu": 0.3, "xi": 0.2, "zeta": 0.1}

A_next = compute_anabolization_factor(A_current, caos_plus, sr, oci, delta_linf, params)
# A_next ‚âà 1.32 (32% acceleration)
```

---

## 15. Œ£-Guard Gate ‚Äî Fail-Closed Validation

**Form:**
```
V_t = ùüô[œÅ<1 ‚àß ECE‚â§0.01 ‚àß œÅ_bias‚â§1.05 ‚àß consent ‚àß eco_ok]
```

**Gates (non-compensatory):**
1. **œÅ < 1**: IR‚ÜíIC contractivity
2. **ECE ‚â§ 0.01**: Calibration (1% error max)
3. **œÅ_bias ‚â§ 1.05**: Fairness (5% max bias)
4. **consent**: Data usage authorization
5. **eco_ok**: Environmental budget
6. **SR ‚â• 0.80**: Metacognition threshold
7. **G ‚â• 0.85**: Global coherence
8. **Œ∫ ‚â• 20**: CAOS‚Å∫ gain minimum
9. **Œ≤_min ‚â• 0.01**: Death gate threshold
10. **Cost ‚â§ budget**: Economic constraint

**Action:** `V_t = 0` ‚Üí rollback + WORM log + reason + suggest fix

**Implementation:**
```python
from penin.equations.sigma_guard_gate import sigma_guard_gate

metrics = {
    "rho": 0.85,
    "ece": 0.008,
    "rho_bias": 1.03,
    "consent": True,
    "eco_ok": True,
    "sr": 0.87,
    "G": 0.91,
    "kappa": 22.0,
    "beta_min": 0.012,
    "cost": 0.15,
    "budget": 0.20,
}

gate_result = sigma_guard_gate(metrics)
# gate_result = {"verdict": True, "passed": 10, "failed": 0}
```

**OPA/Rego Integration:**
```rego
package penin.guard

default allow = false

allow {
    input.rho < 1.0
    input.ece <= 0.01
    input.rho_bias <= 1.05
    input.consent == true
    input.eco_ok == true
    input.sr >= 0.80
    input.G >= 0.85
    input.kappa >= 20.0
    input.beta_min >= 0.01
    input.cost <= input.budget
}
```

---

## Integration Pipeline (Champion‚ÜíChallenger‚ÜíPromote)

**Cycle Flow:**

```
1. MEASURE raw metrics
2. COMPUTE C, A, O, S ‚Üí CAOS‚Å∫
3. EVALUATE Œ£EA/IR‚ÜíIC, SR, EPV, OCI ‚Üí G
4. CALCULATE L‚àû
5. CHECK ŒîL‚àû ‚â• Œ≤_min AND Œ£-Guard (V_t)
6. UPDATE I via Penin Equation (Œ±_t^eff)
7. AUTO-TUNE Œ∫, Œª_c, w_j, Œ≤_min
8. WORM LOG all decisions + hashes
```

**GO/NO-GO Criteria (suggested):**
- Œ∫ ‚â• 20
- Œ≤_min ‚â• 0.01
- U ‚â• 0.90 (utilization)
- œÅ < 1
- ECE ‚â§ 0.01
- œÅ_bias ‚â§ 1.05
- SR ‚â• 0.80
- G ‚â• 0.85
- Cost ‚â§ budget √ó 1.10

---

## Example Numerical Cycle (Toy)

**Inputs:**
- metrics: [0.82, 0.76, 0.94] (accuracy, robust, privacy)
- weights: [0.4, 0.4, 0.2]
- cost: 0.15
- Œª_c: 0.5

**Step 1: L‚àû**
```
L‚àû = [0.4/0.82 + 0.4/0.76 + 0.2/0.94]^(-1) * exp(-0.5*0.15)
   ‚âà 0.796 * 0.927 ‚âà 0.738
```

**Step 2: CAOS‚Å∫**
- C = 0.88 (pass@k=0.9, 1-ECE=0.98, v_ext=0.76)
- A = 0.06/0.15 = 0.40
- O = 0.35
- S = 0.82
- Œ∫ = 20

```
CAOS‚Å∫ = (1 + 20*0.88*0.40)^(0.35*0.82)
      = 8.04^0.287 ‚âà 1.86
```

**Step 3: SR**
```
R_t = harmonic_mean([0.92, 1.0, 0.88, 0.67]) ‚âà 0.84
```

**Step 4: Œ±_eff**
```
Œ±_eff = 0.1 * tanh(0.8*1.86) * 0.84
      ‚âà 0.1 * 0.78 * 0.84 ‚âà 0.065
```

**Step 5: Œ£-Guard**
All gates pass ‚Üí V_t = 1 ‚Üí **PROMOTE**

---

## Best Practices

**Normalization:**
- All metrics ‚Üí [0, 1] via min-max or sigmoid
- EMA smoothing (half-life 3‚Äì10)
- Clamp derivatives and steps

**Non-Compensatory:**
- Use harmonic mean (not arithmetic/geometric)
- Enforce via hard thresholds (Œ£-Guard)

**Fail-Closed:**
- Default deny (Œ£EA/IR‚ÜíIC violations ‚Üí L‚àû=0)
- Rollback atomicity (state + logs + configs)

**Auditability:**
- WORM ledger (append-only, hash-chained)
- PCAg (Proof-Carrying Artifacts) for each promotion
- Timestamp + metrics + reasons + hashes

**Observability:**
- Prometheus metrics: `penin_Linf`, `penin_caos_plus`, `penin_sr`, `penin_rho`, `penin_ece`, `penin_delta_linf`
- Dashboards: Grafana/Jupyter with trend analysis
- Alerts: Œ£-Guard failures, rho violations, cost overruns

---

## FAQ

**Q: Why harmonic mean instead of arithmetic?**  
**A:** Arithmetic allows compensation (high values offset low ones). Harmonic forces **bottleneck dominance** ‚Äî the weakest metric controls the aggregate. This prevents Goodhart's Law exploitation.

**Q: What if all metrics are excellent but one is zero?**  
**A:** L‚àû ‚Üí 0 (fail-closed). This is by design ‚Äî ethical/safety cannot be compensated by performance.

**Q: How to tune Œª_c?**  
**A:** Grid search [0.1, 0.5, 1.0] ‚Üí measure ŒîL‚àû/cost sensitivity ‚Üí auto-tune via AdaGrad (Equation 10).

**Q: What happens during Œ£-Guard failure?**  
**A:** Immediate halt, atomic rollback to last-known-good state, WORM log entry with reason/suggestion, alert operators.

**Q: Can challengers bypass gates?**  
**A:** No. All mutations pass through shadow‚Üícanary‚ÜíŒ£-Guard pipeline. Non-compliant variants are quarantined, not promoted.

---

## References

**Academic:**
- Goodhart's Law & Non-Compensatory Methods (Multi-Criteria Decision Analysis)
- Lyapunov Stability Theory (Control Systems)
- Online Convex Optimization (Hazan et al., 2016)
- Choquet Integral (Fuzzy Measures, Grabisch 1997)

**Implementation:**
- `/workspace/penin/equations/` (all 15 equations)
- `/workspace/tests/test_equations_smoke.py` (smoke tests)
- `/workspace/tests/test_math_core.py` (property-based tests)

**Policies:**
- `/workspace/policies/foundation.yaml` (thresholds)
- `/workspace/policies/rego/*.rego` (OPA rules)

---

**Version:** 1.0.0  
**Last Updated:** 2025-10-01  
**Status:** Production-Ready (122/156 tests passing)  
**License:** Apache 2.0
