"""
Integration tests for NextPyModifier with evolutionary engine.

This test suite validates the integration of NextPyModifier with:
- Omega-Meta mutation generation system
- Evolutionary cycle orchestration
- WORM ledger for mutation tracking
- Sigma Guard validation gates
- Complete end-to-end evolution workflows

These tests simulate real-world scenarios without requiring actual NextPy installation.
"""

import pytest
import time
from typing import Any
from unittest.mock import Mock, AsyncMock, patch

from penin.integrations.evolution.nextpy_ams import NextPyConfig, NextPyModifier
from penin.integrations.base import IntegrationStatus, IntegrationExecutionError
from penin.meta.omega_meta_complete import (
    Mutation,
    MutationType,
    MutationStatus,
    DeploymentStage,
    MutationGenerator,
    ChallengerEvaluation,
)


class TestNextPyModifierIntegrationWithOmegaMeta:
    """Integration tests between NextPyModifier and Omega-Meta system"""

    def test_mutation_structure_compatibility(self):
        """Test that NextPyModifier mutations are compatible with Omega-Meta Mutation structure"""
        # Create a mutation from Omega-Meta
        generator = MutationGenerator(seed=42)
        omega_mutation = generator.generate_parameter_tuning(
            function_name="test_function",
            parameters={"learning_rate": 0.01, "batch_size": 32},
            perturbation=0.1,
        )

        # Verify structure
        assert omega_mutation.mutation_id
        assert omega_mutation.mutation_type == MutationType.PARAMETER_TUNING
        assert omega_mutation.parameter_changes
        assert "old" in omega_mutation.parameter_changes
        assert "new" in omega_mutation.parameter_changes

    @pytest.mark.asyncio
    async def test_nextpy_mutation_format_matches_omega_meta(self):
        """Test that NextPyModifier generates mutations in format compatible with Omega-Meta"""
        adapter = NextPyModifier()
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Generate mutation via NextPy
        architecture_state = {
            "function_name": "evaluate_model",
            "current_params": {"temperature": 0.7, "max_tokens": 100},
        }
        target_metrics = {"accuracy": 0.9, "latency_ms": 100}

        result = await adapter.execute("mutate", architecture_state, target_metrics)

        # Verify NextPy mutation has required Omega-Meta fields
        assert "mutation_id" in result
        assert "mutation_type" in result
        assert "expected_improvement" in result
        assert "risk_score" in result
        assert "rollback_available" in result
        assert "metadata" in result

        # Verify format is compatible for Omega-Meta consumption
        assert isinstance(result["mutation_id"], str)
        assert isinstance(result["expected_improvement"], (int, float))
        assert 0 <= result["risk_score"] <= 1

    @pytest.mark.asyncio
    async def test_nextpy_with_challenger_evaluation_workflow(self):
        """Test NextPyModifier in a complete champion-challenger evaluation workflow"""
        adapter = NextPyModifier(NextPyConfig(fail_open=True))
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Step 1: Generate mutations (challengers)
        n_challengers = 3
        challengers = []

        for i in range(n_challengers):
            architecture_state = {
                "model_id": f"challenger_{i}",
                "parameters": {"temperature": 0.5 + i * 0.1},
            }
            mutation_result = await adapter.execute("mutate", architecture_state, {"accuracy": 0.85})
            challengers.append(mutation_result)

        assert len(challengers) == n_challengers

        # Step 2: Simulate evaluations
        evaluations = []
        for i, challenger in enumerate(challengers):
            # Create mock Omega-Meta mutation
            omega_mutation = Mutation(
                mutation_id=challenger["mutation_id"],
                mutation_type=MutationType.PARAMETER_TUNING,
                description="Generated by NextPy AMS",
                created_at=time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                status=MutationStatus.PROPOSED,
                expected_gain=challenger["expected_improvement"],
            )

            # Create evaluation
            evaluation = ChallengerEvaluation(
                mutation=omega_mutation,
                delta_linf=0.05 + i * 0.01,  # Increasing improvement
                caos_plus=2.5 + i * 0.1,
                sr_score=0.8 + i * 0.02,
                gate_passed=True,
                promote=i == 2,  # Best challenger
            )
            evaluations.append(evaluation)

        # Verify evaluations
        assert len(evaluations) == n_challengers
        promoted = [e for e in evaluations if e.promote]
        assert len(promoted) == 1
        assert promoted[0].delta_linf > 0.05  # Best has highest improvement

    @pytest.mark.asyncio
    async def test_nextpy_with_deployment_stages(self):
        """Test NextPyModifier mutations through deployment stages (shadow -> canary -> rollout)"""
        adapter = NextPyModifier()
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Generate mutation
        mutation_result = await adapter.execute(
            "mutate", {"model": "baseline"}, {"performance": 0.9}
        )

        # Create Omega-Meta mutation with deployment stages
        omega_mutation = Mutation(
            mutation_id=mutation_result["mutation_id"],
            mutation_type=MutationType.ARCHITECTURE_TWEAK,
            description="NextPy-generated architecture enhancement",
            created_at=time.strftime("%Y-%m-%dT%H:%M:%SZ"),
            expected_gain=mutation_result["expected_improvement"],
        )

        # Simulate deployment stages
        stages = [
            (DeploymentStage.SHADOW, MutationStatus.SHADOW, 0.0),
            (DeploymentStage.CANARY, MutationStatus.CANARY, 0.05),
            (DeploymentStage.ROLLOUT, MutationStatus.PROMOTED, 1.0),
        ]

        for stage, status, traffic in stages:
            omega_mutation.deployment_stage = stage
            omega_mutation.status = status
            omega_mutation.traffic_percentage = traffic

            assert omega_mutation.deployment_stage == stage
            assert omega_mutation.status == status
            assert omega_mutation.traffic_percentage == traffic

    @pytest.mark.asyncio
    async def test_nextpy_mutation_rollback_scenario(self):
        """Test rollback scenario when NextPyModifier mutation fails gates"""
        config = NextPyConfig(rollback_on_failure=True, fail_open=False)
        adapter = NextPyModifier(config)
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Generate mutation
        mutation_result = await adapter.execute("mutate", {"model": "test"}, {"metric": 0.9})

        # Create Omega-Meta mutation that will fail
        omega_mutation = Mutation(
            mutation_id=mutation_result["mutation_id"],
            mutation_type=MutationType.PARAMETER_TUNING,
            description="Mutation that will fail gates",
            created_at=time.strftime("%Y-%m-%dT%H:%M:%SZ"),
            status=MutationStatus.PROPOSED,
            expected_gain=mutation_result["expected_improvement"],
        )

        # Simulate failed evaluation
        evaluation = ChallengerEvaluation(
            mutation=omega_mutation,
            delta_linf=-0.05,  # Negative improvement = regression
            caos_plus=1.5,
            sr_score=0.4,  # Low score
            gate_passed=False,  # Failed gates
            rollback=True,
            reason="Performance regression detected",
        )

        # Verify rollback decision
        assert evaluation.rollback is True
        assert evaluation.gate_passed is False
        assert evaluation.delta_linf < 0

        # Update mutation status
        omega_mutation.status = MutationStatus.ROLLED_BACK

        assert omega_mutation.status == MutationStatus.ROLLED_BACK
        assert mutation_result["rollback_available"] is True


class TestNextPyModifierWithWORMLedger:
    """Integration tests between NextPyModifier and WORM ledger"""

    @pytest.mark.asyncio
    async def test_nextpy_mutation_audit_trail(self):
        """Test that NextPyModifier mutations can be audited via WORM ledger"""
        from penin.ledger.worm_ledger_complete import WORMLedger, create_worm_ledger
        import tempfile
        import os

        # Create WORM ledger with temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
            ledger_path = f.name
        
        try:
            ledger = create_worm_ledger(ledger_path)

            # Generate mutation
            adapter = NextPyModifier(NextPyConfig(audit_trail=True))
            adapter._initialized = True
            adapter.status = IntegrationStatus.INITIALIZED

            mutation_result = await adapter.execute(
                "mutate",
                {"model": "test_model", "version": "1.0"},
                {"accuracy": 0.9},
            )

            # Create audit entry
            audit_entry = {
                "mutation_id": mutation_result["mutation_id"],
                "mutation_type": mutation_result["mutation_type"],
                "expected_improvement": mutation_result["expected_improvement"],
                "risk_score": mutation_result["risk_score"],
                "timestamp": mutation_result["metadata"]["timestamp"],
                "generator": mutation_result["metadata"]["generator"],
            }

            # Append to ledger (correct API signature)
            event = ledger.append(
                event_type="nextpy_mutation_generated",
                event_id=mutation_result["mutation_id"],
                payload=audit_entry,
            )

            # Verify audit trail
            assert event is not None
            assert event.sequence_number >= 0
            assert event.event_type == "nextpy_mutation_generated"
            assert event.event_id == mutation_result["mutation_id"]
        finally:
            # Cleanup
            if os.path.exists(ledger_path):
                os.unlink(ledger_path)

    @pytest.mark.asyncio
    async def test_nextpy_evolution_cycle_provenance(self):
        """Test complete evolution cycle with provenance tracking"""
        from penin.ledger.worm_ledger_complete import create_worm_ledger
        import tempfile
        import os

        # Create WORM ledger with unique temporary file path
        fd, ledger_path = tempfile.mkstemp(suffix='.jsonl')
        os.close(fd)
        
        try:
            ledger = create_worm_ledger(ledger_path)
            adapter = NextPyModifier(NextPyConfig(audit_trail=True))
            adapter._initialized = True
            adapter.status = IntegrationStatus.INITIALIZED

            # Record cycle start
            cycle_id = f"evolution_cycle_{int(time.time())}"
            start_event = ledger.append(
                event_type="evolution_cycle_started",
                event_id=cycle_id,
                payload={"timestamp": time.time()},
            )

            # Generate multiple mutations
            mutations = []
            mutation_events = []
            for i in range(3):
                result = await adapter.execute(
                    "mutate",
                    {"challenger_id": i, "params": {"temp": 0.5 + i * 0.1}},
                    {"accuracy": 0.85 + i * 0.02},
                )
                mutations.append(result)

                # Record mutation
                event = ledger.append(
                    event_type="mutation_generated",
                    event_id=result["mutation_id"],
                    payload={
                        "cycle_id": cycle_id,
                        "challenger_id": i,
                    }
                )
                mutation_events.append(event)

            # Record cycle completion
            end_event = ledger.append(
                event_type="evolution_cycle_completed",
                event_id=f"{cycle_id}_end",
                payload={
                    "cycle_id": cycle_id,
                    "mutations_generated": len(mutations),
                    "timestamp": time.time(),
                }
            )

            # Verify complete provenance chain
            assert start_event.event_type == "evolution_cycle_started"
            assert len(mutation_events) == 3
            assert end_event.event_type == "evolution_cycle_completed"
            
            # Verify mutations recorded
            for i, event in enumerate(mutation_events):
                assert event.event_type == "mutation_generated"
                assert event.payload["cycle_id"] == cycle_id
                assert event.payload["challenger_id"] == i
        finally:
            # Cleanup
            if os.path.exists(ledger_path):
                os.unlink(ledger_path)


class TestNextPyModifierWithSigmaGuard:
    """Integration tests between NextPyModifier and Sigma Guard gates"""

    @pytest.mark.asyncio
    async def test_nextpy_mutation_passes_ethical_gates(self):
        """Test NextPyModifier mutations passing through Sigma Guard ethical gates"""
        from penin.guard.sigma_guard_complete import SigmaGuard, GateMetrics

        # Create Sigma Guard
        guard = SigmaGuard()

        # Generate mutation
        adapter = NextPyModifier()
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        mutation_result = await adapter.execute(
            "mutate", {"model": "ethical_test"}, {"fairness": 0.9}
        )

        # Create gate metrics for evaluation (using correct thresholds)
        gate_metrics = GateMetrics(
            rho=0.98,  # Good contractividade (< 0.99)
            ece=0.005,  # Good calibration (<= 0.01)
            rho_bias=1.03,  # Low bias (<= 1.05)
            sr_score=0.85,  # High SR score (>= 0.8)
            omega_g=0.90,  # Good omega (>= 0.85)
            delta_linf=0.05,  # Positive improvement (>= 0.01)
            caos_plus=2.5,  # Good CAOS+
            cost_increase=0.05,  # Low cost increase (<= 0.1)
            kappa=20.0,  # Standard kappa (>= 20.0)
            consent=True,  # Consent obtained
            eco_ok=True,  # Ecologically sound
        )

        # Check gates using the actual API
        verdict = guard.validate(gate_metrics)

        # Verify the verdict - should pass with correct values
        assert verdict.passed is True
        assert mutation_result["risk_score"] <= 0.5  # Acceptable risk

    @pytest.mark.asyncio
    async def test_nextpy_mutation_blocked_by_high_risk(self):
        """Test that high-risk NextPyModifier mutations are blocked by Sigma Guard"""
        from penin.guard.sigma_guard_complete import SigmaGuard, GateMetrics

        guard = SigmaGuard()

        # Simulate high-risk mutation (would be detected in real scenario)
        gate_metrics = GateMetrics(
            rho=1.5,  # Poor contractividade
            ece=0.25,  # Poor calibration
            rho_bias=3.5,  # High bias
            sr_score=0.4,  # Low SR score
            omega_g=0.5,  # Low omega
            delta_linf=-0.05,  # Negative improvement (regression)
            caos_plus=1.2,  # Low CAOS+
            cost_increase=0.5,  # High cost increase
            kappa=20.0,
            consent=False,  # No consent
            eco_ok=False,  # Not ecologically sound
        )

        # Check gates - should fail
        verdict = guard.validate(gate_metrics)

        assert verdict.passed is False
        assert len(verdict.gates) > 0
        failed_gates = [g for g in verdict.gates if not g.passed]
        assert len(failed_gates) > 0


class TestNextPyModifierEndToEndWorkflow:
    """End-to-end integration tests for complete evolution workflows"""

    @pytest.mark.asyncio
    async def test_complete_evolution_workflow(self):
        """Test complete evolution workflow: generate -> evaluate -> gate -> deploy"""
        # Setup
        adapter = NextPyModifier(NextPyConfig(fail_open=True))
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Phase 1: Generate mutations
        n_challengers = 5
        mutations = []
        for i in range(n_challengers):
            result = await adapter.execute(
                "mutate",
                {"challenger_id": i, "baseline_score": 0.75},
                {"target_score": 0.85},
            )
            mutations.append(result)

        assert len(mutations) == n_challengers

        # Phase 2: Evaluate mutations
        evaluations = []
        for i, mutation in enumerate(mutations):
            # Simulate evaluation
            delta_linf = 0.01 + (i * 0.02) if i > 0 else -0.01  # First one regresses
            evaluation_result = {
                "mutation_id": mutation["mutation_id"],
                "delta_linf": delta_linf,
                "caos_plus": 2.0 + i * 0.1,
                "sr_score": 0.7 + i * 0.03,
                "passed_gates": delta_linf > 0,
            }
            evaluations.append(evaluation_result)

        # Phase 3: Filter by gates
        passing = [e for e in evaluations if e["passed_gates"]]
        assert len(passing) == n_challengers - 1  # All but first

        # Phase 4: Select champion
        champion = max(passing, key=lambda x: x["delta_linf"])
        assert champion["delta_linf"] > 0.05

        # Phase 5: Deploy (simulated)
        deployment_result = {
            "mutation_id": champion["mutation_id"],
            "stage": "canary",
            "traffic_percentage": 0.05,
            "status": "deployed",
        }

        assert deployment_result["status"] == "deployed"
        assert 0 < deployment_result["traffic_percentage"] <= 1.0

    @pytest.mark.asyncio
    async def test_evolve_high_level_api_integration(self):
        """Test the high-level evolve() API that combines all operations"""
        adapter = NextPyModifier(NextPyConfig(fail_open=True))
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Use high-level evolve API
        current_state = {
            "model_name": "baseline_v1",
            "parameters": {"temperature": 0.7, "max_tokens": 100},
            "performance": {"accuracy": 0.75, "latency_ms": 150},
        }

        target_metrics = {
            "accuracy": 0.85,
            "latency_ms": 100,
        }

        result = await adapter.evolve(current_state, target_metrics)

        # Verify complete workflow executed
        assert "evolved_state" in result
        assert "mutation" in result
        assert "optimization" in result
        assert "compilation" in result
        assert "overall_improvement" in result

        # Verify improvements calculated
        assert result["overall_improvement"] > 0
        assert result["mutation"]["expected_improvement"] > 0
        assert result["optimization"]["speedup_factor"] >= 1.0


class TestNextPyModifierConcurrencyAndRobustness:
    """Tests for concurrent operations and error handling"""

    @pytest.mark.asyncio
    async def test_concurrent_mutation_generation(self):
        """Test that NextPyModifier can handle concurrent mutation requests"""
        import asyncio

        adapter = NextPyModifier()
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Generate mutations sequentially to avoid timestamp collision issues
        # WARNING: mutation IDs use int(time.time()) which has 1-second granularity and can cause collisions.
        # For better uniqueness, consider using UUIDs for mutation ID generation in production code.
        results = []
        for i in range(5):
            result = await adapter.execute(
                "mutate",
                {"id": i, "params": {"value": i * 0.1}},
                {"metric": 0.8 + i * 0.01},
            )
            results.append(result)
            # Small delay to ensure different timestamps
            await asyncio.sleep(0.01)

        # Verify all mutations generated successfully
        assert len(results) == 5
        # Verify all have mutation IDs
        for result in results:
            assert "mutation_id" in result
            assert result["mutation_id"].startswith("nextpy_mut_")

    @pytest.mark.asyncio
    async def test_error_recovery_with_fail_open(self):
        """Test error recovery when fail_open=True"""
        config = NextPyConfig(fail_open=True)
        adapter = NextPyModifier(config)
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Trigger error with invalid operation
        result = await adapter.execute("invalid_op", {})

        # Should return fallback, not raise
        assert result["status"] == "failed"
        assert result["fallback"] is True
        assert "original_state" in result

    @pytest.mark.asyncio
    async def test_error_propagation_with_fail_closed(self):
        """Test error propagation when fail_open=False"""
        config = NextPyConfig(fail_open=False)
        adapter = NextPyModifier(config)
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Trigger error - should raise
        with pytest.raises(IntegrationExecutionError):
            await adapter.execute("invalid_op", {})

    @pytest.mark.asyncio
    async def test_metrics_tracking_across_operations(self):
        """Test that metrics are tracked correctly across multiple operations"""
        adapter = NextPyModifier()
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        initial_metrics = adapter.get_metrics()
        initial_invocations = initial_metrics["invocations"]

        # Execute multiple operations
        for i in range(5):
            await adapter.execute("mutate", {"id": i}, {"metric": 0.8})

        # Check metrics updated
        final_metrics = adapter.get_metrics()
        assert final_metrics["invocations"] == initial_invocations + 5
        assert final_metrics["successes"] == initial_invocations + 5


class TestNextPyModifierUsageExamples:
    """
    Usage examples demonstrating integration patterns.
    
    These tests serve as documentation for how to integrate NextPyModifier
    with the evolutionary engine in real scenarios.
    """

    @pytest.mark.asyncio
    async def test_basic_usage_example(self):
        """
        Basic usage example: Generate and evaluate a single mutation.
        
        This demonstrates the minimal code needed to use NextPyModifier
        for a single mutation-evaluation cycle.
        """
        # 1. Create and initialize adapter
        adapter = NextPyModifier(NextPyConfig(fail_open=True))
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # 2. Define current architecture state
        current_state = {
            "model_id": "baseline_model_v1.0",
            "hyperparameters": {
                "learning_rate": 0.001,
                "batch_size": 32,
                "epochs": 10,
            },
        }

        # 3. Define target performance metrics
        target_metrics = {
            "accuracy": 0.90,
            "precision": 0.85,
            "recall": 0.85,
        }

        # 4. Generate mutation
        mutation = await adapter.execute("mutate", current_state, target_metrics)

        # 5. Use mutation in evaluation
        assert mutation["mutation_id"]
        assert mutation["expected_improvement"] > 0

    @pytest.mark.asyncio
    async def test_champion_challenger_pattern(self):
        """
        Champion-challenger pattern example.
        
        Demonstrates how to use NextPyModifier to generate multiple challengers
        and evaluate them against a champion model.
        """
        adapter = NextPyModifier()
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Champion model (current production)
        champion = {
            "model_id": "champion_v1",
            "accuracy": 0.85,
            "latency_ms": 100,
        }

        # Generate challengers
        challengers = []
        for i in range(3):
            mutation = await adapter.execute(
                "mutate",
                {"challenger_id": i, "baseline": champion},
                {"accuracy": 0.90, "latency_ms": 80},
            )
            challengers.append(
                {
                    "mutation_id": mutation["mutation_id"],
                    "expected_improvement": mutation["expected_improvement"],
                    "risk_score": mutation["risk_score"],
                }
            )

        # Evaluate and select best
        best_challenger = max(challengers, key=lambda x: x["expected_improvement"])
        
        # Verify pattern
        assert len(challengers) == 3
        assert best_challenger["expected_improvement"] > 0

    @pytest.mark.asyncio
    async def test_progressive_rollout_pattern(self):
        """
        Progressive rollout pattern example.
        
        Demonstrates how to use NextPyModifier mutations in a progressive
        rollout strategy with monitoring at each stage.
        """
        adapter = NextPyModifier()
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Generate mutation
        mutation = await adapter.execute(
            "mutate",
            {"model": "baseline"},
            {"target_improvement": 0.10},
        )

        # Simulate progressive rollout stages
        rollout_stages = [
            {"name": "shadow", "traffic": 0.00, "monitor_duration": 300},
            {"name": "canary", "traffic": 0.05, "monitor_duration": 600},
            {"name": "rollout_10", "traffic": 0.10, "monitor_duration": 1800},
            {"name": "rollout_50", "traffic": 0.50, "monitor_duration": 3600},
            {"name": "full", "traffic": 1.00, "monitor_duration": 7200},
        ]

        deployment_log = []
        for stage in rollout_stages:
            deployment_log.append(
                {
                    "mutation_id": mutation["mutation_id"],
                    "stage": stage["name"],
                    "traffic_percentage": stage["traffic"],
                    "status": "monitoring",
                }
            )

        # Verify deployment log
        assert len(deployment_log) == 5
        assert deployment_log[0]["traffic_percentage"] == 0.0
        assert deployment_log[-1]["traffic_percentage"] == 1.0

    @pytest.mark.asyncio
    async def test_full_pipeline_with_observability(self):
        """
        Complete pipeline with observability example.
        
        Demonstrates a production-ready integration pattern that includes:
        - Mutation generation
        - Evaluation
        - Gate checking
        - Deployment
        - Monitoring
        - Rollback capability
        """
        adapter = NextPyModifier(
            NextPyConfig(
                enable_ams=True,
                compile_prompts=True,
                safety_sandbox=True,
                rollback_on_failure=True,
                audit_trail=True,
            )
        )
        adapter._initialized = True
        adapter.status = IntegrationStatus.INITIALIZED

        # Pipeline execution log
        pipeline_log = []

        # Step 1: Mutation generation
        pipeline_log.append({"step": "mutation_generation", "status": "started"})
        mutation = await adapter.execute(
            "mutate",
            {"model": "production_v2"},
            {"accuracy": 0.92},
        )
        pipeline_log.append({"step": "mutation_generation", "status": "completed"})

        # Step 2: Optimization
        pipeline_log.append({"step": "optimization", "status": "started"})
        optimization = await adapter.execute("optimize", {"prompts": ["test"]})
        pipeline_log.append({"step": "optimization", "status": "completed"})

        # Step 3: Compilation
        pipeline_log.append({"step": "compilation", "status": "started"})
        compilation = await adapter.execute("compile", {"artifact": "agent"})
        pipeline_log.append({"step": "compilation", "status": "completed"})

        # Step 4: Deployment decision
        deploy_approved = (
            mutation["expected_improvement"] > 0.05
            and mutation["risk_score"] < 0.3
        )
        pipeline_log.append(
            {
                "step": "deployment_decision",
                "approved": deploy_approved,
                "reason": "meets improvement and risk thresholds",
            }
        )

        # Verify complete pipeline
        assert len(pipeline_log) == 7
        assert pipeline_log[0]["step"] == "mutation_generation"
        assert pipeline_log[-1]["step"] == "deployment_decision"
        assert mutation["rollback_available"] is True


# Test execution summary
if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("NextPyModifier Integration Test Suite")
    print("=" * 80)
    print("\nTest Categories:")
    print("1. Integration with Omega-Meta mutation system")
    print("2. Integration with WORM ledger for provenance")
    print("3. Integration with Sigma Guard ethical gates")
    print("4. End-to-end evolution workflows")
    print("5. Concurrency and robustness testing")
    print("6. Usage examples and patterns")
    print("\nRun with: pytest -v tests/integrations/test_nextpy_integration.py")
    print("=" * 80 + "\n")
