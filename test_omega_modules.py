#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test suite for PENIN-Ω Omega modules
=====================================

Tests the new modular components:
- Ethics metrics computation
- Guards (Σ-Guard and IR→IC)
- Scoring (L∞, U/S/C/L gates)
- CAOS⁺ computation
- SR-Ω∞ scoring
"""

import sys
import math
import json
from pathlib import Path

# Add workspace to path
sys.path.insert(0, "/workspace")

print("=" * 60)
print("OMEGA MODULES TEST SUITE")
print("=" * 60)


def test_ethics_metrics():
    """Test real ethics metrics computation"""
    print("\n[TEST] Ethics Metrics Computation")

    try:
        from penin.omega.ethics_metrics import EthicsCalculator

        # Test ECE computation
        predictions = [
            (0.9, True),  # High confidence, correct
            (0.8, True),  # High confidence, correct
            (0.7, False),  # Medium confidence, wrong
            (0.3, False),  # Low confidence, wrong
            (0.2, False),  # Low confidence, wrong
        ]

        calc = EthicsCalculator()
        ece, ece_details = calc.calculate_ece(predictions, labels, n_bins=5)
        print(f"  ✓ ECE computed: {ece:.4f}")
        assert 0 <= ece <= 1, "ECE should be in [0,1]"

        # Test bias ratio
        group_outcomes = {"group_a": [0.8, 0.85, 0.9], "group_b": [0.6, 0.65, 0.7], "group_c": [0.75, 0.8, 0.85]}

        calc = EthicsCalculator()
        rho_bias, bias_details = calc.calculate_bias_ratio(predictions, labels, [g for g, _ in groups])
        print(f"  ✓ Bias ratio computed: {rho_bias:.4f}")
        assert rho_bias >= 1.0, "Bias ratio should be >= 1.0"

        # Test risk contractivity
        risk_history = [1.0, 0.95, 0.9, 0.85, 0.8, 0.78]
        calc = EthicsCalculator()
        rho_risk, risk_details = calc.calculate_risk_contraction(risk_history)
        print(f"  ✓ Risk contractivity computed: ρ={rho_risk:.4f}")
        assert rho_risk < 1.0, "Risk should be contractive (ρ < 1)"

        # Test fairness metrics
        predictions_by_group = {
            "group_a": [(0.7, True), (0.8, True), (0.6, False)],
            "group_b": [(0.5, True), (0.4, False), (0.6, True)],
        }

        fairness = compute_fairness_metrics(predictions_by_group)
        print(f"  ✓ Fairness DP: {fairness['demographic_parity']:.4f}")
        print(f"  ✓ Fairness EO: {fairness['equal_opportunity']:.4f}")

        # Test consent validation
        data_sources = [
            {"id": "source1", "consent_given": True, "purpose_specified": True, "retention_defined": True},
            {"id": "source2", "consent_given": True, "purpose_specified": True, "retention_defined": False},
        ]

        consent_valid, consent_details = validate_consent(data_sources)
        print(f"  ✓ Consent validation: {consent_valid}")

        # Test ecological impact
        resource_usage = {"cpu_percent": 45.0, "memory_mb": 2048.0, "gpu_watts": 150.0}

        eco_impact = compute_ecological_impact(resource_usage)
        print(f"  ✓ Ecological impact: {eco_impact:.4f}")
        assert 0 <= eco_impact <= 1, "Eco impact should be in [0,1]"

        print("  ✓ Ethics metrics test PASSED")
        return True

    except ImportError as e:
        print(f"  ✗ Import error: {e}")
        return False
    except Exception as e:
        print(f"  ✗ Test failed: {e}")
        return False


def test_guards():
    """Test Σ-Guard and IR→IC guards"""
    print("\n[TEST] Guards (Σ-Guard and IR→IC)")

    try:
        from penin.omega.guards import sigma_guard, ir_to_ic_contractive, SigmaGuardPolicy, combined_guard_check

        # Test Σ-Guard with passing metrics
        good_metrics = {
            "ece": 0.008,  # Below threshold
            "rho_bias": 1.02,  # Below threshold
            "fairness_dp": 0.05,
            "fairness_eo": 0.08,
            "rho_risk": 0.95,
            "eco_impact": 0.3,
            "consent_valid": True,
            "confidence": 0.85,
            "uncertainty": 0.15,
        }

        policy = SigmaGuardPolicy()
        result = sigma_guard(good_metrics, policy)
        print(f"  ✓ Σ-Guard (good): passed={result.passed}, violations={len(result.violations)}")
        assert result.passed, "Good metrics should pass"

        # Test Σ-Guard with failing metrics
        bad_metrics = {
            "ece": 0.05,  # Above threshold
            "rho_bias": 1.2,  # Above threshold
            "fairness_dp": 0.2,
            "rho_risk": 1.1,  # Not contractive
            "consent_valid": False,
        }

        result = sigma_guard(bad_metrics, policy)
        print(f"  ✓ Σ-Guard (bad): passed={result.passed}, violations={len(result.violations)}")
        assert not result.passed, "Bad metrics should fail"
        assert len(result.violations) > 0, "Should have violations"

        # Test IR→IC contractivity
        contracting_risk = [1.0, 0.95, 0.9, 0.85, 0.8]
        result = ir_to_ic_contractive(contracting_risk)
        print(f"  ✓ IR→IC (contracting): passed={result.passed}")
        assert result.passed, "Contracting risk should pass"

        expanding_risk = [0.8, 0.85, 0.9, 0.95, 1.0]
        result = ir_to_ic_contractive(expanding_risk)
        print(f"  ✓ IR→IC (expanding): passed={result.passed}")
        assert not result.passed, "Expanding risk should fail"

        # Test combined guards
        all_passed, results = combined_guard_check(good_metrics, contracting_risk, policy)
        print(f"  ✓ Combined guards: all_passed={all_passed}")
        assert all_passed, "Good metrics and contracting risk should pass"

        print("  ✓ Guards test PASSED")
        return True

    except ImportError as e:
        print(f"  ✗ Import error: {e}")
        return False
    except Exception as e:
        print(f"  ✗ Test failed: {e}")
        return False


def test_scoring():
    """Test scoring functions (L∞, U/S/C/L)"""
    print("\n[TEST] Scoring Module")

    try:
        from penin.omega.scoring import (
            normalize_series,
            ema,
            linf_harmonic,
            score_gate,
            ScoreVerdict,
            harmonic_mean,
            aggregate_scores,
            compute_delta_linf,
            ScoreTracker,
        )

        # Test normalization
        values = [0.1, 0.5, 0.9, 0.3, 0.7]
        normalized = normalize_series(values, method="minmax")
        print(f"  ✓ Normalization: {normalized[0]:.2f} to {normalized[-1]:.2f}")
        assert all(0 <= v <= 1 for v in normalized), "Should be normalized to [0,1]"

        # Test EMA
        ema_values = ema(values, alpha=0.3)
        print(f"  ✓ EMA computed: {len(ema_values)} values")
        assert len(ema_values) == len(values), "EMA should have same length"

        # Test harmonic mean
        h_mean = harmonic_mean([0.8, 0.6, 0.7])
        print(f"  ✓ Harmonic mean: {h_mean:.4f}")
        assert 0 < h_mean < 1, "Harmonic mean should be in (0,1)"

        # Test L∞ with cost penalty
        metrics = [0.8, 0.7, 0.9, 0.6]
        linf = linf_harmonic(metrics, cost=0.3, lambda_c=0.5, ethical_ok=True)
        print(f"  ✓ L∞ score: {linf:.4f}")
        assert 0 <= linf <= 1, "L∞ should be in [0,1]"

        # Test with ethics veto
        linf_veto = linf_harmonic(metrics, cost=0.3, lambda_c=0.5, ethical_ok=False)
        print(f"  ✓ L∞ with ethics veto: {linf_veto:.4f}")
        assert linf_veto == 0.0, "Ethics veto should zero the score"

        # Test U/S/C/L score gate
        # Note: C (cost) is subtracted in the score calculation
        verdict, score, details = score_gate(
            U=0.8,  # High utility
            S=0.7,  # Good stability
            C=0.3,  # Low cost (good)
            L=0.6,  # Moderate learning
            tau=0.5,  # Adjusted threshold for the scoring formula
        )
        print(f"  ✓ Score gate: {verdict.value}, score={score:.4f}")
        # With the formula: wU*U + wS*S - wC*C + wL*L
        # Expected score ≈ 0.3*0.8 + 0.2*0.7 - 0.3*0.3 + 0.2*0.6 = 0.41
        assert score > 0.3, "Score should be reasonable"

        # Test canary zone
        verdict, score, details = score_gate(U=0.6, S=0.6, C=0.4, L=0.5, tau=0.7, canary_margin=0.1)
        print(f"  ✓ Canary zone: {verdict.value}, score={score:.4f}")

        # Test fail zone
        verdict, score, details = score_gate(U=0.3, S=0.3, C=0.8, L=0.2, tau=0.7)
        print(f"  ✓ Fail zone: {verdict.value}, score={score:.4f}")
        assert verdict == ScoreVerdict.FAIL, "Poor scores should fail"

        # Test delta L∞
        delta, meets = compute_delta_linf(0.75, 0.70, min_delta=0.01)
        print(f"  ✓ ΔL∞: {delta:.4f}, meets threshold: {meets}")
        assert meets, "Delta should meet threshold"

        # Test score tracker
        tracker = ScoreTracker()
        for i in range(5):
            tracker.add(0.5 + i * 0.05)
        stats = tracker.get_stats()
        trend = tracker.get_trend()
        print(f"  ✓ Score tracker: mean={stats['mean']:.3f}, trend={trend}")

        print("  ✓ Scoring test PASSED")
        return True

    except ImportError as e:
        print(f"  ✗ Import error: {e}")
        return False
    except Exception as e:
        print(f"  ✗ Test failed: {e}")
        return False


def test_caos():
    """Test CAOS⁺ computation"""
    print("\n[TEST] CAOS⁺ Module")

    try:
        from penin.omega.caos import (
            compute_caos_plus,
            apply_saturation,
            compute_caos_harmony,
            caos_gradient,
            CAOSConfig,
            CAOSTracker,
        )

        # Test basic CAOS⁺ computation
        C, A, O, S = 0.7, 0.8, 0.6, 0.9
        kappa = 2.0

        caos_value, details = compute_caos_plus(C, A, O, S, kappa)
        print(f"  ✓ CAOS⁺ computed: {caos_value:.4f}")
        assert 0 <= caos_value < 1, "CAOS⁺ should be in [0,1) after saturation"

        # Test with edge cases
        caos_zero, _ = compute_caos_plus(0, 0, 0, 0, kappa)
        print(f"  ✓ CAOS⁺ (all zeros): {caos_zero:.4f}")
        assert caos_zero >= 0, "Should handle zeros gracefully"

        caos_ones, _ = compute_caos_plus(1, 1, 1, 1, kappa)
        print(f"  ✓ CAOS⁺ (all ones): {caos_ones:.4f}")
        assert caos_ones < 1, "Should be saturated below 1"

        # Test saturation function
        saturated = apply_saturation(10.0, gamma=0.5)
        print(f"  ✓ Saturation φ(10): {saturated:.4f}")
        assert 0 <= saturated < 1, "Saturation should map to [0,1)"

        # Test CAOS harmony
        harmony = compute_caos_harmony(C, A, O, S)
        print(f"  ✓ CAOS harmony: {harmony:.4f}")
        assert 0 < harmony <= 1, "Harmony should be in (0,1]"

        # Test gradients
        gradients = caos_gradient(C, A, O, S, kappa)
        print(f"  ✓ CAOS gradients computed: {len(gradients)} components")
        assert "dC" in gradients and "dA" in gradients, "Should have gradients"

        # Test tracker
        tracker = CAOSTracker(alpha=0.3)
        for i in range(5):
            c = 0.5 + i * 0.05
            a = 0.6 + i * 0.04
            o = 0.7 - i * 0.03
            s = 0.8 + i * 0.02
            caos_val, caos_ema = tracker.update(c, a, o, s, kappa)

        stability = tracker.get_stability()
        print(f"  ✓ CAOS tracker: EMA={caos_ema:.4f}, stability={stability:.4f}")

        # Test numerical stability with extreme values
        config = CAOSConfig(kappa_max=100, use_log_space=True)
        caos_extreme, _ = compute_caos_plus(0.99, 0.99, 0.99, 0.99, 50.0, config)
        print(f"  ✓ CAOS⁺ (extreme): {caos_extreme:.4f}")
        assert not math.isnan(caos_extreme), "Should not produce NaN"
        assert not math.isinf(caos_extreme), "Should not produce Inf"

        print("  ✓ CAOS⁺ test PASSED")
        return True

    except ImportError as e:
        print(f"  ✗ Import error: {e}")
        return False
    except Exception as e:
        print(f"  ✗ Test failed: {e}")
        return False


def test_sr():
    """Test SR-Ω∞ computation"""
    print("\n[TEST] SR-Ω∞ Module")

    try:
        from penin.omega.sr import (
            compute_sr_omega,
            harmonic_mean,
            geometric_mean,
            min_soft_pnorm,
            compute_awareness_score,
            compute_autocorrection_score,
            compute_metacognition_score,
            SRConfig,
            SRTracker,
        )

        # Test basic SR computation
        awareness = 0.8
        ethics_ok = True
        autocorrection = 0.7
        metacognition = 0.75

        sr_score, details = compute_sr_omega(awareness, ethics_ok, autocorrection, metacognition)
        print(f"  ✓ SR-Ω∞ computed: {sr_score:.4f}")
        assert 0 <= sr_score <= 1, "SR should be in [0,1]"

        # Test with ethics veto
        sr_veto, details = compute_sr_omega(awareness, False, autocorrection, metacognition)
        print(f"  ✓ SR with ethics veto: {sr_veto:.6f}")
        assert sr_veto < 0.01, "Ethics veto should near-zero SR"

        # Test different aggregation methods
        config_harmonic = SRConfig(aggregation="harmonic")
        sr_h, _ = compute_sr_omega(awareness, ethics_ok, autocorrection, metacognition, config_harmonic)

        config_minsoft = SRConfig(aggregation="min_soft", p_norm=-5.0)
        sr_m, _ = compute_sr_omega(awareness, ethics_ok, autocorrection, metacognition, config_minsoft)

        config_geometric = SRConfig(aggregation="geometric")
        sr_g, _ = compute_sr_omega(awareness, ethics_ok, autocorrection, metacognition, config_geometric)

        print(f"  ✓ Aggregations: harmonic={sr_h:.4f}, min_soft={sr_m:.4f}, geometric={sr_g:.4f}")

        # Test component computations
        internal_state = {"cpu": 0.5, "mem": 0.6, "latency": 0.1}
        awareness_score = compute_awareness_score(internal_state, 0.9, 3)
        print(f"  ✓ Awareness score: {awareness_score:.4f}")

        error_history = [0.5, 0.45, 0.4, 0.38, 0.35]
        autocorr_score = compute_autocorrection_score(error_history, 0.7)
        print(f"  ✓ Autocorrection score: {autocorr_score:.4f}")

        meta_score = compute_metacognition_score(3, 0.7, 0.8)
        print(f"  ✓ Metacognition score: {meta_score:.4f}")

        # Test min-soft p-norm
        values = [0.5, 0.6, 0.7, 0.8]
        min_soft = min_soft_pnorm(values, p=-5.0)
        print(f"  ✓ Min-soft p-norm: {min_soft:.4f}")
        # Min-soft with p=-5 should be close to but slightly above minimum
        assert min_soft <= min(values) * 1.3, "Should approximate minimum"

        # Test tracker
        tracker = SRTracker()
        for i in range(5):
            aw = 0.6 + i * 0.05
            ac = 0.7 - i * 0.02
            mc = 0.65 + i * 0.03
            sr_val, sr_ema = tracker.update(aw, True, ac, mc)

        trend = tracker.get_trend()
        print(f"  ✓ SR tracker: EMA={sr_ema:.4f}, trend={trend}")

        print("  ✓ SR-Ω∞ test PASSED")
        return True

    except ImportError as e:
        print(f"  ✗ Import error: {e}")
        return False
    except Exception as e:
        print(f"  ✗ Test failed: {e}")
        return False


def run_all_tests():
    """Run all omega module tests"""
    results = []

    # Run each test
    results.append(("Ethics Metrics", test_ethics_metrics()))
    results.append(("Guards", test_guards()))
    results.append(("Scoring", test_scoring()))
    results.append(("CAOS⁺", test_caos()))
    results.append(("SR-Ω∞", test_sr()))

    # Summary
    print("\n" + "=" * 60)
    passed = sum(1 for _, result in results if result)
    failed = len(results) - passed

    print(f"RESULTS: {passed} passed, {failed} failed")

    if failed > 0:
        print("\nFailed tests:")
        for name, result in results:
            if not result:
                print(f"  - {name}")

    print("=" * 60)

    return failed == 0


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
