"""
PENIN-Î© Evaluators Module
=========================

Implements evaluation batteries for U/S/C/L metrics (Utility, Stability, Cost, Learning).
Provides deterministic tasks and scoring for model assessment.
"""

import json
import time
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import re


@dataclass
class EvaluationResult:
    """Result of an evaluation"""
    task_id: str
    metric_type: str  # U, S, C, or L
    score: float  # Normalized [0,1]
    raw_metrics: Dict[str, Any]
    evidence: Dict[str, Any]
    latency_ms: float
    cost_usd: float


@dataclass
class TaskBatteryConfig:
    """Configuration for evaluation tasks"""
    seed: int = 42
    timeout_seconds: int = 30
    max_tasks_per_metric: int = 5
    include_synthetic: bool = True
    include_real_world: bool = True


class UtilityEvaluator:
    """Evaluates utility (U) - accuracy, correctness, helpfulness"""
    
    def __init__(self, config: TaskBatteryConfig = None):
        self.config = config or TaskBatteryConfig()
        
    def evaluate_utility(self, model_fn, tasks: List[Dict] = None) -> List[EvaluationResult]:
        """Evaluate model utility across tasks"""
        if tasks is None:
            tasks = self._get_default_utility_tasks()
        
        results = []
        for task in tasks[:self.config.max_tasks_per_metric]:
            start_time = time.time()
            
            try:
                # Run model on task
                response = model_fn(task['input'])
                latency_ms = (time.time() - start_time) * 1000
                
                # Score response
                score = self._score_utility_response(response, task)
                
                # Create result
                result = EvaluationResult(
                    task_id=task['id'],
                    metric_type='U',
                    score=score,
                    raw_metrics={'exact_match': score, 'response_length': len(str(response))},
                    evidence={'task_type': task['type'], 'expected': task['expected']},
                    latency_ms=latency_ms,
                    cost_usd=0.001  # Estimated
                )
                results.append(result)
                
            except Exception as e:
                # Failed task gets 0 score
                result = EvaluationResult(
                    task_id=task['id'],
                    metric_type='U',
                    score=0.0,
                    raw_metrics={'error': str(e)},
                    evidence={'task_type': task['type'], 'failed': True},
                    latency_ms=(time.time() - start_time) * 1000,
                    cost_usd=0.001
                )
                results.append(result)
        
        return results
    
    def _get_default_utility_tasks(self) -> List[Dict]:
        """Get default utility evaluation tasks"""
        return [
            {
                'id': 'math_basic',
                'type': 'arithmetic',
                'input': 'What is 15 + 27?',
                'expected': '42'
            },
            {
                'id': 'json_extract',
                'type': 'structured',
                'input': 'Extract the name from: {"user": {"name": "Alice", "age": 30}}',
                'expected': 'Alice'
            },
            {
                'id': 'text_summary',
                'type': 'summarization',
                'input': 'Summarize in one sentence: The quick brown fox jumps over the lazy dog. This is a common pangram used in typing practice.',
                'expected': 'pangram'  # Key concept should be mentioned
            }
        ]
    
    def _score_utility_response(self, response: str, task: Dict) -> float:
        """Score utility response"""
        response_str = str(response).lower().strip()
        expected = str(task['expected']).lower().strip()
        
        if task['type'] == 'arithmetic':
            # Exact match for math
            return 1.0 if expected in response_str else 0.0
        elif task['type'] == 'structured':
            # Check if expected value is extracted
            return 1.0 if expected in response_str else 0.0
        elif task['type'] == 'summarization':
            # Check if key concept is mentioned
            return 1.0 if expected in response_str else 0.5
        else:
            # Default: partial credit for containing expected
            return 1.0 if expected in response_str else 0.0


class StabilityEvaluator:
    """Evaluates stability (S) - consistency, robustness, calibration"""
    
    def __init__(self, config: TaskBatteryConfig = None):
        self.config = config or TaskBatteryConfig()
    
    def evaluate_stability(self, model_fn, tasks: List[Dict] = None) -> List[EvaluationResult]:
        """Evaluate model stability"""
        if tasks is None:
            tasks = self._get_default_stability_tasks()
        
        results = []
        for task in tasks[:self.config.max_tasks_per_metric]:
            start_time = time.time()
            
            try:
                # Run same task multiple times
                responses = []
                for _ in range(3):  # 3 runs for consistency check
                    response = model_fn(task['input'])
                    responses.append(str(response))
                
                latency_ms = (time.time() - start_time) * 1000
                
                # Score consistency
                score = self._score_stability_responses(responses, task)
                
                result = EvaluationResult(
                    task_id=task['id'],
                    metric_type='S',
                    score=score,
                    raw_metrics={'consistency': score, 'num_runs': len(responses)},
                    evidence={'task_type': task['type'], 'responses': responses},
                    latency_ms=latency_ms,
                    cost_usd=0.003  # 3 runs
                )
                results.append(result)
                
            except Exception as e:
                result = EvaluationResult(
                    task_id=task['id'],
                    metric_type='S',
                    score=0.0,
                    raw_metrics={'error': str(e)},
                    evidence={'task_type': task['type'], 'failed': True},
                    latency_ms=(time.time() - start_time) * 1000,
                    cost_usd=0.003
                )
                results.append(result)
        
        return results
    
    def _get_default_stability_tasks(self) -> List[Dict]:
        """Get default stability tasks"""
        return [
            {
                'id': 'deterministic_math',
                'type': 'deterministic',
                'input': 'What is 2 + 2?',
                'expected_consistency': True
            },
            {
                'id': 'classification',
                'type': 'classification',
                'input': 'Classify sentiment: "I love this product!"',
                'expected_consistency': True
            }
        ]
    
    def _score_stability_responses(self, responses: List[str], task: Dict) -> float:
        """Score response consistency"""
        if len(responses) < 2:
            return 0.0
        
        # Simple consistency: all responses should be similar
        first_response = responses[0].lower().strip()
        consistent_count = sum(1 for r in responses if r.lower().strip() == first_response)
        
        return consistent_count / len(responses)


class CostEvaluator:
    """Evaluates cost (C) - efficiency, resource usage"""
    
    def __init__(self, config: TaskBatteryConfig = None):
        self.config = config or TaskBatteryConfig()
    
    def evaluate_cost(self, model_fn, tasks: List[Dict] = None) -> List[EvaluationResult]:
        """Evaluate model cost efficiency"""
        if tasks is None:
            tasks = self._get_default_cost_tasks()
        
        results = []
        for task in tasks[:self.config.max_tasks_per_metric]:
            start_time = time.time()
            
            try:
                response = model_fn(task['input'])
                latency_ms = (time.time() - start_time) * 1000
                
                # Score based on efficiency (lower latency = higher score)
                score = self._score_cost_efficiency(latency_ms, len(str(response)))
                
                result = EvaluationResult(
                    task_id=task['id'],
                    metric_type='C',
                    score=score,
                    raw_metrics={'latency_ms': latency_ms, 'response_length': len(str(response))},
                    evidence={'task_type': task['type']},
                    latency_ms=latency_ms,
                    cost_usd=latency_ms * 0.00001  # Rough estimate
                )
                results.append(result)
                
            except Exception as e:
                result = EvaluationResult(
                    task_id=task['id'],
                    metric_type='C',
                    score=0.0,
                    raw_metrics={'error': str(e)},
                    evidence={'task_type': task['type'], 'failed': True},
                    latency_ms=(time.time() - start_time) * 1000,
                    cost_usd=0.001
                )
                results.append(result)
        
        return results
    
    def _get_default_cost_tasks(self) -> List[Dict]:
        """Get default cost evaluation tasks"""
        return [
            {
                'id': 'simple_query',
                'type': 'simple',
                'input': 'Hello'
            },
            {
                'id': 'medium_query',
                'type': 'medium',
                'input': 'Explain the concept of machine learning in simple terms'
            }
        ]
    
    def _score_cost_efficiency(self, latency_ms: float, response_length: int) -> float:
        """Score cost efficiency (higher score = more efficient)"""
        # Normalize latency (assume 1000ms is baseline)
        latency_score = max(0.0, 1.0 - (latency_ms / 1000.0))
        
        # Normalize response length (assume 100 chars is good)
        length_score = min(1.0, response_length / 100.0) if response_length > 0 else 0.0
        
        # Combined score (efficiency vs quality trade-off)
        return (latency_score + length_score) / 2.0


class LearningEvaluator:
    """Evaluates learning (L) - adaptation, improvement, tool use"""
    
    def __init__(self, config: TaskBatteryConfig = None):
        self.config = config or TaskBatteryConfig()
    
    def evaluate_learning(self, model_fn, tasks: List[Dict] = None) -> List[EvaluationResult]:
        """Evaluate model learning capabilities"""
        if tasks is None:
            tasks = self._get_default_learning_tasks()
        
        results = []
        for task in tasks[:self.config.max_tasks_per_metric]:
            start_time = time.time()
            
            try:
                response = model_fn(task['input'])
                latency_ms = (time.time() - start_time) * 1000
                
                # Score learning indicators
                score = self._score_learning_response(str(response), task)
                
                result = EvaluationResult(
                    task_id=task['id'],
                    metric_type='L',
                    score=score,
                    raw_metrics={'learning_score': score},
                    evidence={'task_type': task['type']},
                    latency_ms=latency_ms,
                    cost_usd=0.001
                )
                results.append(result)
                
            except Exception as e:
                result = EvaluationResult(
                    task_id=task['id'],
                    metric_type='L',
                    score=0.0,
                    raw_metrics={'error': str(e)},
                    evidence={'task_type': task['type'], 'failed': True},
                    latency_ms=(time.time() - start_time) * 1000,
                    cost_usd=0.001
                )
                results.append(result)
        
        return results
    
    def _get_default_learning_tasks(self) -> List[Dict]:
        """Get default learning tasks"""
        return [
            {
                'id': 'pattern_recognition',
                'type': 'pattern',
                'input': 'What comes next: 2, 4, 8, 16, ?',
                'learning_indicators': ['pattern', 'sequence', 'double']
            },
            {
                'id': 'tool_usage',
                'type': 'tool',
                'input': 'How would you calculate the square root of 144?',
                'learning_indicators': ['calculate', 'math', 'tool']
            }
        ]
    
    def _score_learning_response(self, response: str, task: Dict) -> float:
        """Score learning capabilities"""
        response_lower = response.lower()
        
        if 'learning_indicators' in task:
            indicators = task['learning_indicators']
            found_indicators = sum(1 for indicator in indicators if indicator in response_lower)
            return found_indicators / len(indicators)
        
        # Default: check for reasoning words
        reasoning_words = ['because', 'therefore', 'pattern', 'rule', 'method', 'approach']
        found_reasoning = sum(1 for word in reasoning_words if word in response_lower)
        return min(1.0, found_reasoning / 3.0)  # Cap at 1.0


class TaskBattery:
    """Main evaluation orchestrator"""
    
    def __init__(self, config: TaskBatteryConfig = None):
        self.config = config or TaskBatteryConfig()
        self.utility_evaluator = UtilityEvaluator(config)
        self.stability_evaluator = StabilityEvaluator(config)
        self.cost_evaluator = CostEvaluator(config)
        self.learning_evaluator = LearningEvaluator(config)
    
    def evaluate_all_metrics(self, model_fn) -> Dict[str, Any]:
        """Run complete evaluation battery"""
        results = {
            'U': self.utility_evaluator.evaluate_utility(model_fn),
            'S': self.stability_evaluator.evaluate_stability(model_fn),
            'C': self.cost_evaluator.evaluate_cost(model_fn),
            'L': self.learning_evaluator.evaluate_learning(model_fn)
        }
        
        # Calculate aggregate scores
        aggregates = {}
        for metric_type, evals in results.items():
            if evals:
                scores = [e.score for e in evals]
                aggregates[metric_type] = {
                    'mean': sum(scores) / len(scores),
                    'min': min(scores),
                    'max': max(scores),
                    'count': len(scores)
                }
            else:
                aggregates[metric_type] = {'mean': 0.0, 'min': 0.0, 'max': 0.0, 'count': 0}
        
        return {
            'detailed_results': results,
            'aggregate_scores': aggregates,
            'overall_score': sum(agg['mean'] for agg in aggregates.values()) / 4.0
        }


# Quick evaluation function
def quick_evaluate_model(model_fn, max_tasks: int = 2) -> Dict[str, float]:
    """Quick model evaluation"""
    config = TaskBatteryConfig(max_tasks_per_metric=max_tasks)
    battery = TaskBattery(config)
    results = battery.evaluate_all_metrics(model_fn)
    
    return {
        'U': results['aggregate_scores']['U']['mean'],
        'S': results['aggregate_scores']['S']['mean'],
        'C': results['aggregate_scores']['C']['mean'],
        'L': results['aggregate_scores']['L']['mean'],
        'overall': results['overall_score']
    }
